# åˆ†å¸ƒå¼æ¶æ„å‡çº§ - å½“å‰è¿›åº¦

## ğŸ“Š æ€»ä½“è¿›åº¦: 50% å®Œæˆ

### âœ… å·²å®Œæˆæ¨¡å—

#### 1. æ¶æ„è®¾è®¡ âœ“
- **æ–‡æ¡£**: `docs/ARCHITECTURE_DESIGN.md`
- **çŠ¶æ€**: å®Œæˆ
- **å†…å®¹**:
  - å®Œæ•´çš„ç³»ç»Ÿæ¶æ„è®¾è®¡
  - æ•°æ®æµå®šä¹‰
  - Topicè®¾è®¡
  - API Keyæ± è®¾è®¡
  - å®¹é‡è§„åˆ’
  - ç›‘æ§æŒ‡æ ‡å®šä¹‰

#### 2. Kafkaæ¶ˆæ¯æ¨¡å‹ âœ“
- **æ–‡ä»¶**: `streaming/kafka_messages.py`
- **çŠ¶æ€**: å®Œæˆ
- **åŠŸèƒ½**:
  - `DetectionResultMessage`: æ£€æµ‹ç»“æœæ¶ˆæ¯æ¨¡å‹
  - `AssessmentTaskMessage`: è¯„ä¼°ä»»åŠ¡æ¶ˆæ¯æ¨¡å‹
  - `RiskAssessmentResultMessage`: é£é™©è¯„ä¼°ç»“æœæ¶ˆæ¯æ¨¡å‹
  - æ¶ˆæ¯åºåˆ—åŒ–/ååºåˆ—åŒ–
  - ç±»å‹å®‰å…¨çš„æ•°æ®ç»“æ„

#### 3. Kafkaé…ç½® âœ“
- **æ–‡ä»¶**: `streaming/kafka_config.py`
- **çŠ¶æ€**: å®Œæˆ
- **åŠŸèƒ½**:
  - 4ä¸ªTopicå®šä¹‰ï¼ˆdetection-results, assessment-tasks, risk-results, dlqï¼‰
  - Produceré…ç½®ç”Ÿæˆå™¨
  - Consumeré…ç½®ç”Ÿæˆå™¨
  - æ”¯æŒSASL/SSLå®‰å…¨è®¤è¯

#### 4. API Keyæ± ç®¡ç†å™¨ âœ“
- **æ–‡ä»¶**: `streaming/api_key_pool.py`
- **çŠ¶æ€**: å®Œæˆ
- **åŠŸèƒ½**:
  - KeyçŠ¶æ€ç®¡ç†ï¼ˆAVAILABLE, IN_USE, COOLING, DISABLEDï¼‰
  - å¤šç§è°ƒåº¦ç­–ç•¥ï¼ˆè½®è¯¢ã€æœ€å°‘è´Ÿè½½ã€ä¼˜å…ˆçº§ï¼‰
  - é€Ÿç‡é™åˆ¶æ£€æµ‹
  - è‡ªåŠ¨å†·å´æœºåˆ¶
  - å¥åº·æ£€æŸ¥API
  - å®æ—¶ç»Ÿè®¡ä¿¡æ¯

#### 5. ä»»åŠ¡è°ƒåº¦å™¨ âœ“
- **æ–‡ä»¶**: `streaming/task_scheduler.py`
- **çŠ¶æ€**: å®Œæˆ
- **åŠŸèƒ½**:
  - Kafkaæ¶ˆè´¹è€…ï¼ˆä»assessment-tasks-topicï¼‰
  - Kafkaç”Ÿäº§è€…ï¼ˆåˆ°risk-results-topicï¼‰
  - å¼‚æ­¥Workeræ± 
  - LLM APIè°ƒç”¨å°è£…
  - é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
  - æ­»ä¿¡é˜Ÿåˆ—ï¼ˆDLQï¼‰æ”¯æŒ
  - æ€§èƒ½ç»Ÿè®¡

#### 6. å®æ–½æŒ‡å— âœ“
- **æ–‡ä»¶**: `docs/IMPLEMENTATION_GUIDE.md`
- **çŠ¶æ€**: å®Œæˆ
- **å†…å®¹**:
  - å®Œæ•´çš„å®æ–½æ­¥éª¤
  - æµ‹è¯•è®¡åˆ’
  - ç›‘æ§æ–¹æ¡ˆ
  - è¿ç»´æ‰‹å†Œ
  - æ•…éšœæ’æŸ¥æŒ‡å—

---

## ğŸš§ å¾…å®Œæˆæ¨¡å—

### 1. Kafka Produceré€‚é…å™¨ (ä¼˜å…ˆçº§: HIGH)
**æ–‡ä»¶**: `streaming/kafka_producer.py`

**ä»»åŠ¡**:
```python
class DetectionKafkaProducer:
    """Adapter for sending detection results to Kafka."""
    
    def __init__(self, config: KafkaSettings):
        self.producer = KafkaProducer(...)
    
    def send_detection_result(self, pipeline_payload: Dict):
        """Convert pipeline payload to Kafka message."""
        message = self._convert_to_kafka_message(pipeline_payload)
        self.producer.send(DETECTION_RESULTS_TOPIC.name, message)
```

**é¢„è®¡å·¥æ—¶**: 4å°æ—¶

---

### 2. æ”¹é€ æ£€æµ‹ç®¡é“ (ä¼˜å…ˆçº§: HIGH)
**æ–‡ä»¶**: `algo/rtsp_detect/pipeline.py`

**ä»»åŠ¡**:
- æ·»åŠ Kafka Producerä½œä¸ºå¯é€‰ç»„ä»¶
- ä¿®æ”¹`_run()`æ–¹æ³•ï¼Œåœ¨æ¨é€WebSocketå‰å‘é€åˆ°Kafka
- æ·»åŠ é…ç½®å¼€å…³ï¼ˆå‘åå…¼å®¹ï¼‰
- è½¬æ¢æ•°æ®æ ¼å¼åˆ°Kafkaæ¶ˆæ¯

**ä»£ç ç¤ºä¾‹**:
```python
class DetectionPipeline:
    def __init__(self, ..., kafka_producer=None, use_kafka=False):
        self.kafka_producer = kafka_producer
        self.use_kafka = use_kafka
    
    def _run(self):
        # ... ç°æœ‰æ£€æµ‹é€»è¾‘ ...
        
        # æ–°å¢ï¼šå‘é€åˆ°Kafka
        if self.use_kafka and self.kafka_producer:
            kafka_message = self._create_kafka_message(
                detected_objects, groups, raw_frame
            )
            self.kafka_producer.send_detection_result(kafka_message)
        
        # ä¿æŒåŸæœ‰WebSocketæ¨é€
        if self.callback:
            self.callback(payload)
```

**é¢„è®¡å·¥æ—¶**: 8å°æ—¶

---

### 3. Flinkæµå¤„ç†ä½œä¸š (ä¼˜å…ˆçº§: MEDIUM)
**æ–‡ä»¶**: `streaming/flink_processor.py`

**ä»»åŠ¡**:
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import (
    FlinkKafkaConsumer,
    FlinkKafkaProducer
)

def create_traffic_assessment_job():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(32)
    env.enable_checkpointing(10000)  # 10s
    
    # Source
    detection_consumer = FlinkKafkaConsumer(
        topics='detection-results-topic',
        deserialization_schema=...,
        properties={...}
    )
    
    detection_stream = env.add_source(detection_consumer)
    
    # Transform: å±•å¼€ç¾¤ç»„ä¸ºç‹¬ç«‹ä»»åŠ¡
    task_stream = detection_stream.flat_map(lambda msg: [
        create_assessment_task(msg, group) 
        for group in msg.traffic_groups
        if should_assess(group)
    ])
    
    # Sink
    task_producer = FlinkKafkaProducer(
        topic='assessment-tasks-topic',
        serialization_schema=...,
        producer_config={...}
    )
    
    task_stream.add_sink(task_producer)
    
    env.execute("TrafficRiskAssessmentJob")
```

**é¢„è®¡å·¥æ—¶**: 12å°æ—¶

**æ³¨æ„**: å¯ä»¥æš‚æ—¶è·³è¿‡Flinkï¼Œç›´æ¥åœ¨è°ƒåº¦å™¨ä¸­æ¶ˆè´¹detection-results-topic

---

### 4. ç»“æœæ¶ˆè´¹æœåŠ¡ (ä¼˜å…ˆçº§: HIGH)
**æ–‡ä»¶**: `streaming/result_consumer.py`

**ä»»åŠ¡**:
```python
class RiskResultConsumer:
    """Consume risk assessment results and push to WebSocket."""
    
    def __init__(self, websocket_manager):
        self.ws_manager = websocket_manager
        self.consumer = KafkaConsumer(
            RISK_ASSESSMENT_RESULTS_TOPIC.name,
            ...
        )
        self._result_cache = {}  # camera_id -> results
    
    async def start(self):
        """Start consuming results."""
        while True:
            messages = self.consumer.poll(timeout_ms=1000)
            for tp, records in messages.items():
                for record in records:
                    result = RiskAssessmentResultMessage.from_dict(record.value)
                    await self._handle_result(result)
    
    async def _handle_result(self, result: RiskAssessmentResultMessage):
        """Process a single result."""
        # èšåˆåŒä¸€æ‘„åƒå¤´çš„ç»“æœ
        camera_id = result.camera_id
        if camera_id not in self._result_cache:
            self._result_cache[camera_id] = []
        
        self._result_cache[camera_id].append(result)
        
        # å½“æ”¶é›†åˆ°æ‰€æœ‰ç¾¤ç»„ç»“æœæ—¶ï¼Œæ¨é€åˆ°WebSocket
        if self._is_complete(camera_id):
            await self._push_to_websocket(camera_id)
```

**é¢„è®¡å·¥æ—¶**: 6å°æ—¶

---

### 5. Docker Composeé…ç½® (ä¼˜å…ˆçº§: MEDIUM)
**æ–‡ä»¶**: `docker/docker-compose.yml`

**ä»»åŠ¡**:
```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
  
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  
  flink-jobmanager:
    image: flink:1.18
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
  
  flink-taskmanager:
    image: flink:1.18
    depends_on:
      - flink-jobmanager
    command: taskmanager
    scale: 4
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
  
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

**é¢„è®¡å·¥æ—¶**: 4å°æ—¶

---

### 6. åˆå§‹åŒ–è„šæœ¬ (ä¼˜å…ˆçº§: MEDIUM)
**æ–‡ä»¶**: `scripts/setup_kafka.sh` å’Œ `scripts/setup_kafka.py`

**ä»»åŠ¡**:
```bash
#!/bin/bash
# setup_kafka.sh

echo "Creating Kafka topics..."

kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic detection-results-topic \
  --partitions 16 \
  --replication-factor 3 \
  --config retention.ms=3600000 \
  --config compression.type=snappy

kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic assessment-tasks-topic \
  --partitions 32 \
  --replication-factor 3 \
  --config retention.ms=7200000

kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic risk-assessment-results-topic \
  --partitions 16 \
  --replication-factor 3 \
  --config retention.ms=86400000

echo "Topics created successfully!"
```

**é¢„è®¡å·¥æ—¶**: 2å°æ—¶

---

### 7. ç›‘æ§é›†æˆ (ä¼˜å…ˆçº§: LOW)
**æ–‡ä»¶**: 
- `streaming/metrics.py`: PrometheusæŒ‡æ ‡å¯¼å‡º
- `config/prometheus.yml`: Prometheusé…ç½®
- `config/grafana_dashboard.json`: Grafanaä»ªè¡¨ç›˜

**ä»»åŠ¡**:
- é›†æˆprometheus-client
- å¯¼å‡ºå…³é”®æŒ‡æ ‡
- åˆ›å»ºGrafanaä»ªè¡¨ç›˜æ¨¡æ¿

**é¢„è®¡å·¥æ—¶**: 6å°æ—¶

---

### 8. é…ç½®æ–‡ä»¶ (ä¼˜å…ˆçº§: HIGH)
**æ–‡ä»¶**: 
- `config/api_keys.yaml`: API Keysé…ç½®
- `config/kafka.yaml`: Kafkaè¿æ¥é…ç½®

**ç¤ºä¾‹**:
```yaml
# config/api_keys.yaml
strategy: round_robin  # round_robin | least_loaded | priority
max_retry_per_key: 3
default_cooldown: 60.0

keys:
  - api_key: ${DASHSCOPE_KEY_001}
    name: production-key-001
    qps_limit: 5
    rpm_limit: 100
    priority: 1
  
  - api_key: ${DASHSCOPE_KEY_002}
    name: production-key-002
    qps_limit: 5
    rpm_limit: 100
    priority: 1
  
  # æ·»åŠ æ›´å¤škeys...
```

**é¢„è®¡å·¥æ—¶**: 2å°æ—¶

---

## ğŸ“… å®æ–½æ—¶é—´çº¿

### ç¬¬ä¸€é˜¶æ®µ (æœ¬å‘¨)
- [x] æ¶æ„è®¾è®¡æ–‡æ¡£
- [x] Kafkaæ¶ˆæ¯æ¨¡å‹
- [x] API Keyæ± 
- [x] ä»»åŠ¡è°ƒåº¦å™¨
- [ ] Kafka Produceré€‚é…å™¨
- [ ] é…ç½®æ–‡ä»¶

### ç¬¬äºŒé˜¶æ®µ (ä¸‹å‘¨)
- [ ] æ”¹é€ æ£€æµ‹ç®¡é“
- [ ] ç»“æœæ¶ˆè´¹æœåŠ¡
- [ ] Docker Compose
- [ ] åˆå§‹åŒ–è„šæœ¬

### ç¬¬ä¸‰é˜¶æ®µ (ç¬¬ä¸‰å‘¨)
- [ ] Flinkæµå¤„ç†ä½œä¸šï¼ˆå¯é€‰ï¼‰
- [ ] ç›‘æ§é›†æˆ
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] æ–‡æ¡£å®Œå–„

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼ˆå½“å‰å¯ç”¨ï¼‰

è™½ç„¶ç³»ç»Ÿæœªå®Œå…¨å®ç°ï¼Œä½†æ ¸å¿ƒç»„ä»¶å·²å¯ç‹¬ç«‹æµ‹è¯•ï¼š

### æµ‹è¯•API Key Pool
```python
from streaming.api_key_pool import APIKeyPool, APIKeyInfo, create_key_pool_from_config
import asyncio

# åˆ›å»ºæµ‹è¯•é…ç½®
config = {
    "strategy": "round_robin",
    "keys": [
        {"api_key": "test-key-1", "name": "key-1", "qps_limit": 5},
        {"api_key": "test-key-2", "name": "key-2", "qps_limit": 5},
    ]
}

pool = create_key_pool_from_config(config)

async def test():
    # è·å–key
    key = await pool.acquire_key()
    print(f"Acquired: {key.name}")
    
    # ä½¿ç”¨åé‡Šæ”¾
    await pool.release_key(key, success=True)
    
    # æŸ¥çœ‹ç»Ÿè®¡
    stats = await pool.health_check()
    print(stats)

asyncio.run(test())
```

### æµ‹è¯•æ¶ˆæ¯æ¨¡å‹
```python
from streaming.kafka_messages import DetectionResultMessage, DetectedObject, TrafficGroup

# åˆ›å»ºæ£€æµ‹ç»“æœæ¶ˆæ¯
message = DetectionResultMessage(
    camera_id=1,
    detected_objects=[
        DetectedObject(
            object_id=0,
            class_name="car",
            confidence=0.85,
            bbox=[100, 200, 150, 250]
        )
    ],
    traffic_groups=[
        TrafficGroup(
            group_index=1,
            object_count=2,
            bbox=[90, 180, 170, 280],
            classes=["car", "person"],
            avg_confidence=0.82,
            member_indices=[0, 1],
            group_image_base64="base64_data_here"
        )
    ]
)

# åºåˆ—åŒ–
data = message.to_dict()
print(json.dumps(data, indent=2))

# ååºåˆ—åŒ–
restored = DetectionResultMessage.from_dict(data)
assert restored.camera_id == message.camera_id
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚éœ€ç»§ç»­å®ç°å‰©ä½™æ¨¡å—ï¼Œè¯·å‘ŠçŸ¥ä¼˜å…ˆçº§ï¼Œæˆ‘å¯ä»¥ç»§ç»­å®Œæˆï¼š

1. **é«˜ä¼˜å…ˆçº§**: Kafka Producer + Pipelineæ”¹é€  â†’ å¿«é€ŸéªŒè¯ç«¯åˆ°ç«¯æµç¨‹
2. **ä¸­ä¼˜å…ˆçº§**: ç»“æœæ¶ˆè´¹æœåŠ¡ â†’ å®Œæˆé—­ç¯
3. **ä½ä¼˜å…ˆçº§**: Flinkä½œä¸š â†’ é«˜çº§æµå¤„ç†ï¼ˆå¯æš‚æ—¶è·³è¿‡ï¼‰

---

**å½“å‰çŠ¶æ€**: æ ¸å¿ƒæ¶æ„å’ŒåŸºç¡€ç»„ä»¶å·²å®Œæˆï¼Œå¯ä»¥å¼€å§‹é›†æˆæµ‹è¯• âœ¨
