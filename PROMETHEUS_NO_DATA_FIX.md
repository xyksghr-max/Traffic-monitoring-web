# Prometheus LLM æŒ‡æ ‡æ— æ•°æ®é—®é¢˜ä¿®å¤

## ğŸ› é—®é¢˜æè¿°

åœ¨ Prometheus ä¸­æ‰§è¡ŒæŸ¥è¯¢ï¼š
```promql
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m]))
```

**ç»“æœ**: æ²¡æœ‰æ•°æ®è¿”å›

---

## ğŸ” é—®é¢˜æ ¹æœ¬åŸå› 

### åŸå›  1: **æŒ‡æ ‡æœªè¢«è®°å½•** âš ï¸

`DangerousDrivingAnalyzer` åœ¨è°ƒç”¨ LLM API åï¼Œ**æ²¡æœ‰è°ƒç”¨ `record_llm_request()`** å°†æŒ‡æ ‡è®°å½•åˆ° Prometheusã€‚

**é—®é¢˜ä»£ç ** (`algo/llm/dangerous_driving_detector.py`):
```python
# âŒ ç¼ºå°‘ Prometheus é›†æˆ
response = self._client.chat.completions.create(...)
parsed["latency"] = time.time() - start_time
return parsed  # æ²¡æœ‰è®°å½•åˆ° Prometheusï¼
```

### åŸå›  2: **éœ€è¦å®é™…çš„ LLM è°ƒç”¨**

å³ä½¿ä¿®å¤äº†ä»£ç ï¼Œhistogram æŒ‡æ ‡ä¹Ÿåªåœ¨**çœŸæ­£è°ƒç”¨ LLM** æ—¶æ‰ä¼šäº§ç”Ÿæ•°æ®ï¼š

**è§¦å‘æ¡ä»¶**:
1. âœ… å¯åŠ¨æ‘„åƒå¤´æµï¼ˆWebSocket è¿æ¥ï¼‰
2. âœ… æ£€æµ‹åˆ° â‰¥2 ä¸ªå¯¹è±¡ æˆ– å½¢æˆäº¤é€šç»„
3. âœ… æ»¡è¶³ cooldown é—´éš”ï¼ˆé»˜è®¤ 3 ç§’ï¼‰
4. âœ… LLM åˆ†æåŠŸèƒ½å·²å¯ç”¨ï¼ˆ`model_config.yaml` ä¸­ `llm.enabled: true`ï¼‰
5. âœ… `DASHSCOPE_API_KEY` å·²é…ç½®

**å¦‚æœæ²¡æœ‰å®é™… LLM è°ƒç”¨ï¼Œå°±ä¸ä¼šæœ‰æ•°æ®ï¼**

---

## âœ… è§£å†³æ–¹æ¡ˆ

### ä¿®å¤ 1: é›†æˆ Prometheus æŒ‡æ ‡è®°å½•

**å·²ä¿®å¤æ–‡ä»¶**: `algo/llm/dangerous_driving_detector.py`

#### 1.1 å¯¼å…¥ metrics æ¨¡å—

```python
# Import Prometheus metrics (optional)
try:
    from algo.monitoring.metrics import record_llm_request
    METRICS_AVAILABLE = True
except ImportError:
    METRICS_AVAILABLE = False
    logger.warning("Prometheus metrics not available for LLM monitoring")
```

#### 1.2 è®°å½•æˆåŠŸçš„è¯·æ±‚

```python
# åœ¨ LLM è°ƒç”¨æˆåŠŸå
latency = time.time() - start_time

if METRICS_AVAILABLE:
    usage = getattr(response, 'usage', None)
    prompt_tokens = getattr(usage, 'prompt_tokens', 0) if usage else 0
    completion_tokens = getattr(usage, 'completion_tokens', 0) if usage else 0
    
    record_llm_request(
        model=self.config.model,
        api_key_id='default',
        latency=latency,
        status='success',
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens
    )
```

#### 1.3 è®°å½•å¤±è´¥çš„è¯·æ±‚

```python
# æ•è·å¼‚å¸¸æ—¶
except Exception as exc:
    latency = time.time() - start_time
    if METRICS_AVAILABLE:
        record_llm_request(
            model=self.config.model,
            api_key_id='default',
            latency=latency,
            status='error'
        )
```

---

### ä¿®å¤ 2: è§¦å‘ LLM è°ƒç”¨ä»¥ç”Ÿæˆæ•°æ®

#### æ–¹æ³• 1: ä½¿ç”¨çœŸå®æ‘„åƒå¤´æµï¼ˆæ¨èï¼‰

```powershell
# 1. å¯åŠ¨ Flask åº”ç”¨
python app.py

# 2. æ‰“å¼€å‰ç«¯é¡µé¢
# è®¿é—®: http://localhost:5000

# 3. æ·»åŠ æ‘„åƒå¤´å¹¶å¯åŠ¨
# - ç‚¹å‡»"æ·»åŠ æ‘„åƒå¤´"
# - è¾“å…¥ RTSP URLï¼ˆæˆ–ä½¿ç”¨æµ‹è¯•è§†é¢‘ï¼‰
# - ç‚¹å‡»"å¼€å§‹æ£€æµ‹"

# 4. ç­‰å¾…æ£€æµ‹åˆ°å¯¹è±¡
# - è‡³å°‘éœ€è¦ 2 ä¸ªå¯¹è±¡æˆ–å½¢æˆäº¤é€šç»„
# - æ¯æ¬¡ LLM è°ƒç”¨é—´éš” 3 ç§’
```

#### æ–¹æ³• 2: ä½¿ç”¨æµ‹è¯•è§†é¢‘æ–‡ä»¶

```powershell
# å‡†å¤‡æµ‹è¯•è§†é¢‘
# å°†åŒ…å«è½¦è¾†/è¡Œäººçš„è§†é¢‘æ–‡ä»¶æ”¾åˆ°é¡¹ç›®ä¸­

# åœ¨å‰ç«¯ RTSP URL è¾“å…¥æ¡†ä¸­ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
# ä¾‹å¦‚: /path/to/test_traffic.mp4
```

#### æ–¹æ³• 3: ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆå¼€å‘æµ‹è¯•ï¼‰

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_llm_metrics.py`:

```python
import time
from algo.llm.dangerous_driving_detector import DangerousDrivingAnalyzer, DangerousDrivingConfig
from algo.monitoring.metrics import record_llm_request

# é…ç½®
config = DangerousDrivingConfig(
    model="qwen-vl-plus",
    timeout=30,
    max_retry=2
)

analyzer = DangerousDrivingAnalyzer(config, enabled=True)

# æ¨¡æ‹Ÿæ£€æµ‹æ•°æ®
detections = [
    {"class": "car", "confidence": 0.95, "bbox": [100, 100, 200, 200]},
    {"class": "person", "confidence": 0.92, "bbox": [150, 150, 180, 250]},
]

groups = [
    {
        "groupIndex": 1,
        "objectCount": 2,
        "classes": ["car", "person"],
        "bbox": [100, 100, 200, 250],
        "memberIndices": [0, 1]
    }
]

group_images = [
    {
        "groupIndex": 1,
        "objectCount": 2,
        "classes": ["car", "person"],
        "dataUri": "data:image/jpeg;base64,..."  # éœ€è¦å®é™…å›¾ç‰‡
    }
]

# è§¦å‘ LLM åˆ†æ
if analyzer.should_analyze(detections, groups, group_images):
    result = analyzer.analyze(group_images, detections, groups)
    print(f"LLM Analysis Result: {result}")
    print(f"Latency: {result.get('latency', 0):.2f}s")
```

è¿è¡Œæµ‹è¯•ï¼š
```powershell
python test_llm_metrics.py
```

---

## ğŸ” éªŒè¯ä¿®å¤

### 1. æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¢«è®°å½•

è®¿é—® Flask metrics ç«¯ç‚¹ï¼š
```powershell
curl http://localhost:5000/metrics | Select-String "llm_latency"
```

**é¢„æœŸè¾“å‡º**ï¼ˆå¦‚æœæœ‰ LLM è°ƒç”¨ï¼‰:
```
# HELP llm_latency_seconds LLM API request latency in seconds
# TYPE llm_latency_seconds histogram
llm_latency_seconds_bucket{api_key_id="default",le="0.5",model="qwen-vl-plus"} 0.0
llm_latency_seconds_bucket{api_key_id="default",le="1.0",model="qwen-vl-plus"} 1.0
llm_latency_seconds_bucket{api_key_id="default",le="2.0",model="qwen-vl-plus"} 3.0
llm_latency_seconds_bucket{api_key_id="default",le="5.0",model="qwen-vl-plus"} 5.0
llm_latency_seconds_bucket{api_key_id="default",le="10.0",model="qwen-vl-plus"} 5.0
llm_latency_seconds_bucket{api_key_id="default",le="30.0",model="qwen-vl-plus"} 5.0
llm_latency_seconds_bucket{api_key_id="default",le="+Inf",model="qwen-vl-plus"} 5.0
llm_latency_seconds_count{api_key_id="default",model="qwen-vl-plus"} 5.0
llm_latency_seconds_sum{api_key_id="default",model="qwen-vl-plus"} 12.3456
```

**å¦‚æœæ²¡æœ‰è¾“å‡º**ï¼Œè¯´æ˜ï¼š
- âŒ æ²¡æœ‰å‘ç”Ÿ LLM è°ƒç”¨
- âŒ æŒ‡æ ‡è®°å½•ä»£ç æœªæ‰§è¡Œ

---

### 2. åœ¨ Prometheus ä¸­æŸ¥è¯¢

è®¿é—® Prometheus UI: http://localhost:9100

#### åŸºç¡€æŸ¥è¯¢ï¼ˆæŸ¥çœ‹åŸå§‹æ•°æ®ï¼‰

```promql
# æŸ¥çœ‹æ˜¯å¦æœ‰ llm_latency æŒ‡æ ‡
llm_latency_seconds_bucket

# æŸ¥çœ‹ LLM è¯·æ±‚æ€»æ•°
llm_requests_total

# æŸ¥çœ‹æˆåŠŸçš„è¯·æ±‚æ•°
llm_requests_total{status="success"}
```

**å¦‚æœè¿”å› "No data"**:
1. æ£€æŸ¥ Prometheus æ˜¯å¦æ­£å¸¸æŠ“å– Flask metrics
2. è®¿é—® **Status â†’ Targets**ï¼Œç¡®è®¤ `flask-app` çŠ¶æ€ä¸º **UP**
3. ç¡®è®¤å·²ç»è§¦å‘äº† LLM è°ƒç”¨ï¼ˆæ£€æŸ¥ Flask æ—¥å¿—ï¼‰

#### P95 å»¶è¿ŸæŸ¥è¯¢

```promql
# P95 å»¶è¿Ÿï¼ˆéœ€è¦è‡³å°‘æœ‰ä¸€äº›æ•°æ®ç‚¹ï¼‰
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m]))

# å¹³å‡å»¶è¿Ÿ
rate(llm_latency_seconds_sum[5m]) / rate(llm_latency_seconds_count[5m])

# è¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
rate(llm_requests_total{status="success"}[1m])
```

---

### 3. æŸ¥çœ‹ Flask æ—¥å¿—

å¯åŠ¨åº”ç”¨æ—¶ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
2025-10-20 21:40:00.123 | INFO     | algo.monitoring.metrics:<module>:260 - Prometheus metrics initialized
2025-10-20 21:40:05.456 | INFO     | app:create_app:45 - Prometheus metrics endpoint enabled at /metrics
```

è§¦å‘ LLM åˆ†ææ—¶ï¼š
```
2025-10-20 21:40:30.789 | INFO     | algo.llm.dangerous_driving_detector:analyze:115 - DashScope API call successful, latency=2.34s
```

---

## ğŸ“Š å®Œæ•´ç›‘æ§æµç¨‹

### 1. å¯åŠ¨ç›‘æ§åŸºç¡€è®¾æ–½

```powershell
cd deployment
docker-compose -f docker-compose.infra.yml up -d prometheus grafana
```

### 2. å¯åŠ¨ Flask åº”ç”¨

```powershell
# ç¡®ä¿å®‰è£…äº† prometheus-client
pip install prometheus-client>=0.19.0

# å¯åŠ¨åº”ç”¨
python app.py
```

### 3. éªŒè¯ Prometheus è¿æ¥

è®¿é—® http://localhost:9100/targets

ç¡®è®¤ `flask-app` ç›®æ ‡ï¼š
- **State**: UP âœ…
- **Labels**: `job="flask-app"`
- **Last Scrape**: å‡ ç§’å‰

### 4. è§¦å‘ LLM è°ƒç”¨

**æ–¹å¼ 1**: é€šè¿‡ WebSocket å¯åŠ¨æ‘„åƒå¤´

```javascript
// åœ¨æµè§ˆå™¨æ§åˆ¶å°
ws = new WebSocket('ws://localhost:5000/ws');
ws.onopen = () => {
    ws.send(JSON.stringify({
        type: 'start_stream',
        data: {
            cameraId: 1,
            rtspUrl: 'rtsp://your-camera-url'
        }
    }));
};
```

**æ–¹å¼ 2**: ç›´æ¥è°ƒç”¨ Python API

```python
from algo.llm.dangerous_driving_detector import DangerousDrivingAnalyzer, DangerousDrivingConfig

config = DangerousDrivingConfig()
analyzer = DangerousDrivingAnalyzer(config)

# å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆéœ€è¦å®é™…å›¾ç‰‡ï¼‰
result = analyzer.analyze(group_images, detections, groups)
```

### 5. æŸ¥è¯¢ Prometheus æŒ‡æ ‡

ç­‰å¾… 1-2 åˆ†é’Ÿåï¼ˆè®© Prometheus æŠ“å–æ•°æ®ï¼‰ï¼Œæ‰§è¡ŒæŸ¥è¯¢ï¼š

```promql
# æŸ¥çœ‹æ‰€æœ‰ LLM æŒ‡æ ‡
{__name__=~"llm_.*"}

# P50/P95/P99 å»¶è¿Ÿ
histogram_quantile(0.50, rate(llm_latency_seconds_bucket[5m]))
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m]))
histogram_quantile(0.99, rate(llm_latency_seconds_bucket[5m]))

# LLM é”™è¯¯ç‡
rate(llm_requests_total{status!="success"}[5m]) / rate(llm_requests_total[5m])
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: `/metrics` è¿”å›ç©ºçš„ LLM æŒ‡æ ‡

**æ£€æŸ¥æ¸…å•**:
```powershell
# 1. ç¡®è®¤æŒ‡æ ‡å·²æ³¨å†Œ
curl http://localhost:5000/metrics | Select-String "llm_"

# 2. æ£€æŸ¥æ˜¯å¦æœ‰ LLM è°ƒç”¨
# æŸ¥çœ‹ Flask æ—¥å¿—ï¼Œæœç´¢ "DashScope"

# 3. ç¡®è®¤ Prometheus metrics å·²å¯¼å…¥
python -c "from algo.monitoring.metrics import llm_latency; print(llm_latency)"
```

**åŸå› **: æ²¡æœ‰è§¦å‘ LLM è°ƒç”¨ï¼Œæˆ–ä»£ç ä¸­æ²¡æœ‰è°ƒç”¨ `record_llm_request()`

---

### é—®é¢˜ 2: Prometheus æŸ¥è¯¢è¿”å› "No datapoints found"

**å¯èƒ½åŸå› **:

#### A. æ—¶é—´èŒƒå›´å¤ªçŸ­
```promql
# âŒ 5åˆ†é’Ÿå†…å¯èƒ½æ²¡æœ‰è¶³å¤Ÿæ•°æ®
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[5m]))

# âœ… å°è¯•æ›´é•¿æ—¶é—´èŒƒå›´
histogram_quantile(0.95, rate(llm_latency_seconds_bucket[1h]))
```

#### B. Histogram bucket æ²¡æœ‰æ•°æ®
```promql
# å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½• bucket æ•°æ®
llm_latency_seconds_bucket

# æ£€æŸ¥æ€»è¯·æ±‚æ•°
llm_latency_seconds_count
```

#### C. æ ‡ç­¾ä¸åŒ¹é…
```promql
# âŒ å¦‚æœæŒ‡å®šäº†ä¸å­˜åœ¨çš„æ ‡ç­¾
histogram_quantile(0.95, rate(llm_latency_seconds_bucket{model="gpt-4"}[5m]))

# âœ… å…ˆæŸ¥çœ‹å¯ç”¨æ ‡ç­¾
llm_latency_seconds_bucket{model="qwen-vl-plus"}
```

---

### é—®é¢˜ 3: Histogram åªæœ‰ä¸€ä¸ªæ•°æ®ç‚¹

**åŸå› **: LLM è°ƒç”¨é¢‘ç‡å¤ªä½

**è§£å†³æ–¹æ¡ˆ**:
1. é™ä½ cooldown æ—¶é—´ï¼ˆ`model_config.yaml`ï¼‰:
   ```yaml
   llm:
     cooldown_seconds: 1.0  # ä» 3.0 å‡å°‘åˆ° 1.0
   ```

2. å¯åŠ¨å¤šä¸ªæ‘„åƒå¤´æµ

3. ä½¿ç”¨å¾ªç¯è„šæœ¬ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼š
   ```python
   import time
   for i in range(10):
       result = analyzer.analyze(...)
       print(f"Request {i+1}: latency={result['latency']:.2f}s")
       time.sleep(2)
   ```

---

## ğŸ“ˆ Grafana ä»ªè¡¨æ¿é…ç½®

### æ·»åŠ  LLM å»¶è¿Ÿé¢æ¿

1. è®¿é—® Grafana: http://localhost:3100
2. ç™»å½•ï¼ˆadmin/adminï¼‰
3. åˆ›å»ºæ–° Dashboard
4. æ·»åŠ  Panelï¼Œé…ç½®æŸ¥è¯¢ï¼š

**P95 å»¶è¿Ÿ**:
```promql
histogram_quantile(0.95, sum(rate(llm_latency_seconds_bucket[5m])) by (le, model))
```

**å¹³å‡å»¶è¿Ÿ**:
```promql
rate(llm_latency_seconds_sum[5m]) / rate(llm_latency_seconds_count[5m])
```

**è¯·æ±‚æˆåŠŸç‡**:
```promql
sum(rate(llm_requests_total{status="success"}[5m])) 
/ 
sum(rate(llm_requests_total[5m])) * 100
```

**å»¶è¿Ÿçƒ­åŠ›å›¾ï¼ˆHeatmapï¼‰**:
```promql
sum(increase(llm_latency_seconds_bucket[1m])) by (le)
```

---

## âœ… éªŒè¯æ¸…å•

å®Œæˆä¿®å¤åï¼Œç¡®è®¤ä»¥ä¸‹é¡¹ï¼š

- [ ] `algo/llm/dangerous_driving_detector.py` å·²å¯¼å…¥ `record_llm_request`
- [ ] æˆåŠŸçš„ LLM è°ƒç”¨ä¼šè®°å½• `status='success'`
- [ ] å¤±è´¥çš„ LLM è°ƒç”¨ä¼šè®°å½• `status='error'`
- [ ] Token ä½¿ç”¨é‡è¢«æ­£ç¡®æå–å’Œè®°å½•
- [ ] Flask `/metrics` ç«¯ç‚¹æ˜¾ç¤º `llm_latency_seconds_bucket` æŒ‡æ ‡
- [ ] Prometheus èƒ½æˆåŠŸæŠ“å– Flask metricsï¼ˆTargets é¡µé¢æ˜¾ç¤º UPï¼‰
- [ ] è‡³å°‘è§¦å‘äº†å‡ æ¬¡ LLM è°ƒç”¨ï¼ˆæ£€æŸ¥ `llm_requests_total` > 0ï¼‰
- [ ] Prometheus æŸ¥è¯¢ `llm_latency_seconds_bucket` è¿”å›æ•°æ®
- [ ] `histogram_quantile(...)` æŸ¥è¯¢è¿”å›æœ‰æ•ˆå€¼ï¼ˆéç©ºï¼‰

---

## ğŸ¯ å¿«é€Ÿæµ‹è¯•å‘½ä»¤

```powershell
# 1. æ£€æŸ¥ Flask metrics
curl http://localhost:5000/metrics | Select-String "llm_"

# 2. æ£€æŸ¥ Prometheus è¿æ¥
curl http://localhost:9100/api/v1/targets | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty activeTargets | Where-Object { $_.labels.job -eq "flask-app" }

# 3. æŸ¥è¯¢ LLM è¯·æ±‚æ€»æ•°
curl "http://localhost:9100/api/v1/query?query=llm_requests_total" | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty result

# 4. æŸ¥è¯¢ P95 å»¶è¿Ÿ
curl "http://localhost:9100/api/v1/query?query=histogram_quantile(0.95,%20rate(llm_latency_seconds_bucket[5m]))" | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty result
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [PROMETHEUS_METRICS_FIX.md](PROMETHEUS_METRICS_FIX.md) - Metrics ç«¯ç‚¹é…ç½®
- [algo/monitoring/metrics.py](algo/monitoring/metrics.py) - æŒ‡æ ‡å®šä¹‰
- [Prometheus Histogram](https://prometheus.io/docs/practices/histograms/) - å®˜æ–¹æ–‡æ¡£

---

**ä¿®å¤å®Œæˆï¼** ğŸ‰

ç°åœ¨ LLM è°ƒç”¨ä¼šè‡ªåŠ¨è®°å½•åˆ° Prometheusï¼Œå¯ä»¥æŸ¥è¯¢å»¶è¿Ÿåˆ†ä½æ•°äº†ã€‚

**é‡è¦æç¤º**: å¿…é¡»æœ‰**å®é™…çš„ LLM è°ƒç”¨**æ‰ä¼šäº§ç”Ÿæ•°æ®ï¼Œè¯·ç¡®ä¿ï¼š
1. âœ… æ‘„åƒå¤´æµæ­£åœ¨è¿è¡Œ
2. âœ… æ£€æµ‹åˆ°è¶³å¤Ÿçš„å¯¹è±¡
3. âœ… LLM åŠŸèƒ½å·²å¯ç”¨
4. âœ… API Key å·²é…ç½®
