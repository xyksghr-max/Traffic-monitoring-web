"""Result Aggregator - consumes risk assessment results and aggregates with detection data."""

from __future__ import annotations

import json
import threading
from typing import Dict, Any, Optional, List, TYPE_CHECKING
from pathlib import Path
import sys
from collections import defaultdict

import redis
from loguru import logger

if TYPE_CHECKING:
    from redis import Redis

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(ROOT_DIR))

from algo.kafka.base_consumer import BaseKafkaConsumer


class ResultAggregator:
    """
    ç»“æœèšåˆå™¨
    
    åŠŸèƒ½ï¼š
    1. æ¶ˆè´¹é£é™©è¯„ä¼°ç»“æœ
    2. ä¸åŸå§‹æ£€æµ‹ç»“æœå…³è”
    3. ç¼“å­˜åˆ° Redis
    4. å‘å¸ƒåˆ° WebSocket é¢‘é“
    5. è§¦å‘å‘Šè­¦
    """
    
    def __init__(
        self, 
        kafka_bootstrap: str,
        redis_host: str = 'localhost',
        redis_port: int = 6379,
        redis_db: int = 0,
        enable_redis: bool = True
    ):
        """
        åˆå§‹åŒ–ç»“æœèšåˆå™¨
        
        Args:
            kafka_bootstrap: Kafka æœåŠ¡å™¨åœ°å€
            redis_host: Redis ä¸»æœº
            redis_port: Redis ç«¯å£
            redis_db: Redis æ•°æ®åº“ç¼–å·
            enable_redis: æ˜¯å¦å¯ç”¨ Redis
        """
        self.enable_redis = enable_redis
        self.redis_client: Optional[Redis] = None
        
        # æ·»åŠ ï¼šæŒ‰ requestId èšåˆå¤šä¸ªç¾¤ç»„çš„ç»“æœ
        self.pending_results: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.result_locks: Dict[str, threading.Lock] = defaultdict(threading.Lock)
        self.expected_groups: Dict[str, int] = {}
        
        # åˆå§‹åŒ– Redis
        if enable_redis:
            try:
                self.redis_client = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=True,
                    socket_connect_timeout=5,
                    socket_timeout=5
                )
                # æµ‹è¯•è¿æ¥
                self.redis_client.ping()
                logger.info(f"Redis connected: {redis_host}:{redis_port}/{redis_db}")
            except Exception as e:
                logger.error(f"Failed to connect to Redis: {e}")
                logger.warning("Redis functionality will be disabled")
                self.enable_redis = False
                self.redis_client = None
        
        # åˆå§‹åŒ– Kafka æ¶ˆè´¹è€…
        self.consumer = BaseKafkaConsumer(
            bootstrap_servers=kafka_bootstrap,
            group_id="result-aggregator",
            topics=["risk-assessment-results"],
            message_handler=self.handle_assessment_result
        )
        
        logger.info("Result Aggregator initialized")
    
    def handle_assessment_result(self, result: Dict[str, Any]):
        """
        å¤„ç†è¯„ä¼°ç»“æœï¼ˆèšåˆåŒä¸€ requestId çš„æ‰€æœ‰ç¾¤ç»„ï¼‰
        
        Args:
            result: å•ä¸ªç¾¤ç»„çš„è¯„ä¼°ç»“æœ
        """
        try:
            request_id = result.get('requestId')
            camera_id = result.get('cameraId')
            group_index = result.get('groupIndex')
            
            group_risk = result.get('results', [{}])[0].get('riskLevel', 'none') if result.get('results') else 'none'
            logger.info(
                "ğŸ“¥ Received LLM assessment: requestId={}, cameraId={}, groupIndex={}, risk={}",
                request_id,
                camera_id,
                group_index,
                group_risk
            )
            
            if not camera_id:
                logger.warning("Assessment result missing cameraId, skipping")
                return
            
            if not request_id:
                logger.warning("Assessment result missing requestId, skipping")
                return
            
            # 1. ä» Redis è·å–åŸå§‹æ£€æµ‹ç»“æœ
            detection_data = self._get_detection_from_redis(request_id)
            if not detection_data:
                logger.warning(
                    "âŒ Detection data not found in Redis for requestId={}, cannot aggregate",
                    request_id
                )
                # å³ä½¿æ²¡æœ‰æ£€æµ‹æ•°æ®ï¼Œä¹Ÿå°è¯•å‘å¸ƒè¯„ä¼°ç»“æœ
                self._publish_single_result_without_detection(result, camera_id)
                return
            
            # 2. è·å–é¢„æœŸçš„ç¾¤ç»„æ•°é‡
            expected_count = len(detection_data.get('trafficGroups', []))
            logger.debug(
                "Expected {} groups for requestId={}, current group={}",
                expected_count,
                request_id,
                group_index
            )
            
            # 3. èšåˆè¯¥ç¾¤ç»„çš„ç»“æœ
            with self.result_locks[request_id]:
                self.pending_results[request_id].append(result)
                self.expected_groups[request_id] = expected_count
                
                current_count = len(self.pending_results[request_id])
                logger.debug(
                    "Collected {}/{} group results for requestId={}",
                    current_count,
                    expected_count,
                    request_id
                )
                
                # 4. å¦‚æœæ‰€æœ‰ç¾¤ç»„ç»“æœéƒ½æ”¶åˆ°äº†ï¼Œè¿›è¡Œèšåˆå¹¶å‘å¸ƒ
                if current_count >= expected_count:
                    all_results = self.pending_results.pop(request_id)
                    self.expected_groups.pop(request_id)
                    self.result_locks.pop(request_id)
                    
                    logger.info(
                        "âœ… All {} group results collected for requestId={}, aggregating...",
                        expected_count,
                        request_id
                    )
                    
                    # 5. èšåˆæ‰€æœ‰ç¾¤ç»„çš„ç»“æœ
                    merged_result = self._merge_all_group_results(
                        detection_data,
                        all_results
                    )
                    
                    # 6. ç¼“å­˜æœ€æ–°ç»“æœåˆ° Redis
                    if self.enable_redis and self.redis_client:
                        self._cache_latest_result(camera_id, merged_result)
                    
                    # 7. å‘å¸ƒåˆ° WebSocket é¢‘é“
                    if self.enable_redis and self.redis_client:
                        self._publish_to_websocket(camera_id, merged_result)
                    
                    # 8. è§¦å‘é«˜é£é™©å‘Šè­¦
                    max_risk = merged_result.get('maxRiskLevel', 'none')
                    if max_risk == 'high':
                        self._trigger_alert(camera_id, merged_result)
                    
                    logger.info(
                        "âœ… Aggregated and published result for camera {}, risk={}, hasDangerous={}",
                        camera_id,
                        max_risk,
                        merged_result.get('hasDangerousDriving', False)
                    )
                else:
                    logger.debug(
                        "â³ Waiting for more results: {}/{} collected for requestId={}",
                        current_count,
                        expected_count,
                        request_id
                    )
            
        except Exception as e:
            logger.error(f"Failed to aggregate result: {e}", exc_info=True)
    
    def _merge_all_group_results(
        self,
        detection_data: Dict[str, Any],
        all_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        èšåˆæ‰€æœ‰ç¾¤ç»„çš„è¯„ä¼°ç»“æœ
        
        Args:
            detection_data: åŸå§‹æ£€æµ‹æ•°æ®
            all_results: æ‰€æœ‰ç¾¤ç»„çš„LLMè¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å®Œæ•´ç»“æœ
        """
        # æ”¶é›†æ‰€æœ‰ç¾¤ç»„çš„é£é™©ç­‰çº§
        all_group_results = []
        max_risk = "none"
        risk_order = {"none": 0, "low": 1, "medium": 2, "high": 3}
        max_llm_latency = 0.0
        llm_model = ""
        
        for result in all_results:
            # æå–è¯¥ç¾¤ç»„çš„ç»“æœ
            group_results = result.get('results', [])
            all_group_results.extend(group_results)
            
            # æ›´æ–°æœ€å¤§é£é™©ç­‰çº§
            for group_result in group_results:
                risk = group_result.get('riskLevel', 'none')
                if risk_order.get(risk, 0) > risk_order.get(max_risk, 0):
                    max_risk = risk
            
            # æå– metadata
            metadata = result.get('metadata', {})
            latency = metadata.get('llmLatency', 0.0)
            if latency > max_llm_latency:
                max_llm_latency = latency
            if not llm_model and metadata.get('llmModel'):
                llm_model = metadata.get('llmModel', '')
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å±é™©é©¾é©¶ï¼ˆä»»ä¸€ç¾¤ç»„é£é™© != noneï¼‰
        has_dangerous = max_risk != "none"
        
        logger.debug(
            "Aggregated {} group results: maxRisk={}, hasDangerous={}",
            len(all_group_results),
            max_risk,
            has_dangerous
        )
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        return {
            **detection_data,
            'riskAssessment': {
                'allGroupResults': all_group_results,
                'resultCount': len(all_group_results),
                'groupCount': len(all_results),
            },
            'hasDangerousDriving': has_dangerous,
            'maxRiskLevel': max_risk,
            'llmLatency': max_llm_latency,
            'llmModel': llm_model,
        }
    
    def _publish_single_result_without_detection(
        self,
        result: Dict[str, Any],
        camera_id: int
    ):
        """
        åœ¨æ²¡æœ‰æ£€æµ‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå‘å¸ƒå•ä¸ªç¾¤ç»„çš„ç»“æœ
        ï¼ˆé™çº§å¤„ç†ï¼Œä¸æ¨èï¼‰
        """
        try:
            # æå–ç¾¤ç»„é£é™©
            group_risk = result.get('results', [{}])[0].get('riskLevel', 'none') if result.get('results') else 'none'
            
            minimal_result = {
                'cameraId': camera_id,
                'timestamp': result.get('timestamp'),
                'riskAssessment': {
                    'allGroupResults': result.get('results', []),
                    'resultCount': len(result.get('results', [])),
                    'warning': 'Detection data not found, showing partial results'
                },
                'hasDangerousDriving': group_risk != 'none',
                'maxRiskLevel': group_risk,
                'llmLatency': result.get('metadata', {}).get('llmLatency', 0.0),
                'llmModel': result.get('metadata', {}).get('llmModel', ''),
            }
            
            if self.enable_redis and self.redis_client:
                self._publish_to_websocket(camera_id, minimal_result)
                logger.warning(
                    "âš ï¸  Published partial result without detection data: camera={}, risk={}",
                    camera_id,
                    group_risk
                )
        except Exception as e:
            logger.error(f"Failed to publish single result: {e}")
    
    def _get_detection_from_redis(self, request_id: str) -> Optional[Dict[str, Any]]:
        """ä» Redis è·å–æ£€æµ‹ç»“æœ"""
        try:
            detection_key = f"detection:{request_id}"
            detection_json = self.redis_client.get(detection_key)
            if detection_json:
                logger.debug(
                    "âœ… Found detection data in Redis: key={}, size={} bytes",
                    detection_key,
                    len(detection_json)
                )
                return json.loads(detection_json)
            else:
                logger.warning(
                    "âŒ Detection data not found in Redis: key={}, requestId={}",
                    detection_key,
                    request_id
                )
                return None
        except Exception as e:
            logger.error(f"Failed to get detection from Redis: {e}")
            return None
    
    def _cache_latest_result(self, camera_id: int, result: Dict[str, Any]):
        """ç¼“å­˜æœ€æ–°ç»“æœåˆ° Redis"""
        try:
            camera_key = f"camera:{camera_id}:latest"
            result_json = json.dumps(result, ensure_ascii=False)
            # è®¾ç½® 5 åˆ†é’Ÿè¿‡æœŸ
            self.redis_client.setex(camera_key, 300, result_json)
            logger.debug(f"Cached latest result for camera {camera_id}")
        except Exception as e:
            logger.error(f"Failed to cache result to Redis: {e}")
    
    def _publish_to_websocket(self, camera_id: int, result: Dict[str, Any]):
        """å‘å¸ƒç»“æœåˆ° WebSocket é¢‘é“"""
        try:
            channel = f"camera:{camera_id}"
            result_json = json.dumps(result, ensure_ascii=False)
            subscribers = self.redis_client.publish(channel, result_json)
            logger.info(
                "ğŸ“¡ Published to WebSocket channel '{}': {} subscribers, risk={}, hasDangerous={}",
                channel,
                subscribers,
                result.get('maxRiskLevel', 'none'),
                result.get('hasDangerousDriving', False)
            )
        except Exception as e:
            logger.error(f"Failed to publish to WebSocket: {e}")
    
    def _trigger_alert(self, camera_id: int, result: Dict[str, Any]):
        """è§¦å‘é«˜é£é™©å‘Šè­¦"""
        try:
            alert_key = f"alerts:{camera_id}"
            alert_json = json.dumps(result, ensure_ascii=False)
            
            # æ·»åŠ åˆ°å‘Šè­¦åˆ—è¡¨ï¼ˆæœ€å¤šä¿ç•™ 100 æ¡ï¼‰
            if self.enable_redis and self.redis_client:
                self.redis_client.lpush(alert_key, alert_json)
                self.redis_client.ltrim(alert_key, 0, 99)
            
            logger.warning(f"ğŸš¨ High risk alert triggered for camera {camera_id}")
            
            # TODO: å‘é€å‘Šè­¦é€šçŸ¥ï¼ˆé‚®ä»¶ã€çŸ­ä¿¡ã€Webhook ç­‰ï¼‰
            
        except Exception as e:
            logger.error(f"Failed to trigger alert: {e}")
    
    def start(self):
        """å¯åŠ¨èšåˆå™¨"""
        logger.info("Starting Result Aggregator...")
        thread = threading.Thread(target=self.consumer.start, daemon=True)
        thread.start()
        logger.success("Result Aggregator started")
        return thread
    
    def stop(self):
        """åœæ­¢èšåˆå™¨"""
        logger.info("Stopping Result Aggregator...")
        self.consumer.stop()
        if self.redis_client:
            self.redis_client.close()
        logger.info("Result Aggregator stopped")
