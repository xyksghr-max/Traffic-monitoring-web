"""LLM Task Scheduler with API Key pooling and concurrent execution."""

from __future__ import annotations

import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
import sys

from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(ROOT_DIR))

from algo.scheduler.api_key_pool import APIKeyPool, APIKey
from algo.kafka.base_consumer import BaseKafkaConsumer
from algo.kafka.detection_producer import DetectionResultProducer
from algo.llm.prompts import DANGEROUS_DRIVING_PROMPT


class LLMTaskScheduler:
    """
    LLM ä»»åŠ¡è°ƒåº¦å™¨
    
    åŠŸèƒ½ï¼š
    1. ä»Ž Kafka æ¶ˆè´¹è¯„ä¼°ä»»åŠ¡
    2. ä½¿ç”¨ API Key æ± å¹¶å‘è°ƒç”¨å¤§æ¨¡åž‹
    3. å°†è¯„ä¼°ç»“æžœå‘é€åˆ° Kafka
    """
    
    def __init__(
        self,
        key_pool: APIKeyPool,
        kafka_bootstrap: str,
        input_topic: str = "assessment-tasks",
        output_topic: str = "risk-assessment-results",
        max_concurrent_tasks: int = 50,
        llm_model: str = "qwen-vl-plus",
        llm_timeout: int = 30,
        max_retries: int = 2
    ):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            key_pool: API Key æ± 
            kafka_bootstrap: Kafka æœåŠ¡å™¨åœ°å€
            input_topic: è¾“å…¥ Topic (è¯„ä¼°ä»»åŠ¡)
            output_topic: è¾“å‡º Topic (è¯„ä¼°ç»“æžœ)
            max_concurrent_tasks: æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
            llm_model: LLM æ¨¡åž‹åç§°
            llm_timeout: LLM è¯·æ±‚è¶…æ—¶æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.key_pool = key_pool
        self.max_concurrent_tasks = max_concurrent_tasks
        self.llm_model = llm_model
        self.llm_timeout = llm_timeout
        self.max_retries = max_retries
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        
        # ä¿¡å·é‡æŽ§åˆ¶å¹¶å‘æ•°
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        
        # Kafka æ¶ˆè´¹è€…
        self.task_consumer = BaseKafkaConsumer(
            bootstrap_servers=kafka_bootstrap,
            group_id="llm-task-scheduler",
            topics=[input_topic],
            message_handler=self._handle_task_sync
        )
        
        # Kafka ç”Ÿäº§è€…
        self.result_producer = DetectionResultProducer(
            bootstrap_servers=kafka_bootstrap,
            topic=output_topic
        )
        
        self.running = False
        self.pending_tasks = asyncio.Queue()
        self.event_loop: Optional[asyncio.AbstractEventLoop] = None
        
        logger.info(f"LLM Task Scheduler initialized (max_concurrent={max_concurrent_tasks})")
    
    def _handle_task_sync(self, task_data: Dict[str, Any]):
        """åŒæ­¥å¤„ç†ä»»åŠ¡ï¼ˆä»Ž Kafka Consumer è°ƒç”¨ï¼‰"""
        logger.info(
            "ðŸ“¥ Received LLM task: taskId={}, requestId={}, cameraId={}",
            task_data.get('taskId'),
            task_data.get('requestId'),
            task_data.get('cameraId')
        )
        if self.event_loop and self.running:
            asyncio.run_coroutine_threadsafe(
                self.pending_tasks.put(task_data),
                self.event_loop
            )
    
    async def call_llm_api(
        self,
        api_key: APIKey,
        task: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šçš„ API Key è°ƒç”¨å¤§æ¨¡åž‹ API
        
        Args:
            api_key: API Key
            task: è¯„ä¼°ä»»åŠ¡
            
        Returns:
            è¯„ä¼°ç»“æžœ
        """
        headers = {
            "Authorization": f"Bearer {api_key.key}",
            "Content-Type": "application/json"
        }
        
        # æž„é€ è¯·æ±‚payload
        group_image = task.get('groupImageBase64', '')
        prompt = self._build_prompt(task)
        
        payload = {
            "model": self.llm_model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a professional traffic safety analyst."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": group_image}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        }
        
        for attempt in range(self.max_retries + 1):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.llm_timeout)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            return self._parse_llm_response(result, task)
                        elif response.status == 429:  # Rate limit
                            logger.warning(
                                f"Rate limit hit for {api_key.key_id}, "
                                f"attempt {attempt + 1}/{self.max_retries + 1}"
                            )
                            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        else:
                            error_text = await response.text()
                            logger.error(f"LLM API error {response.status}: {error_text[:200]}")
                            if attempt == self.max_retries:
                                raise Exception(f"API call failed: HTTP {response.status}")
            
            except asyncio.TimeoutError:
                logger.error(f"LLM API timeout for {api_key.key_id}, attempt {attempt + 1}")
                if attempt == self.max_retries:
                    raise
            except Exception as e:
                logger.error(f"LLM API exception: {e}")
                if attempt == self.max_retries:
                    raise
                await asyncio.sleep(1)
        
        return self._empty_result(task)
    
    async def process_task(self, task: Dict[str, Any]):
        """å¤„ç†å•ä¸ªè¯„ä¼°ä»»åŠ¡"""
        task_id = task.get('taskId', 'unknown')
        
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            api_key = None
            start_time = time.time()
            
            try:
                # èŽ·å– API Key
                api_key = self.key_pool.acquire_key(timeout=10.0)
                if not api_key:
                    logger.error(f"Failed to acquire API key for task {task_id}")
                    error_result = self._error_result(task, "No available API key")
                    self._send_result(error_result, task.get('cameraId', 0))
                    return
                
                # è°ƒç”¨ LLM API
                result = await self.call_llm_api(api_key, task)
                
                # æ‰“å°å¤§æ¨¡åž‹è¿”å›žçš„å®Œæ•´ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
                logger.info(
                    "ðŸ¤– LLM Response for task {}: riskLevel={}, hasDangerous={}, results={}",
                    task_id,
                    result.get('maxRiskLevel', 'none'),
                    result.get('hasDangerousDriving', False),
                    json.dumps(result.get('results', []), ensure_ascii=False)[:500]
                )
                
                # é‡Šæ”¾ Key (æˆåŠŸ)
                self.key_pool.release_key(api_key, success=True)
                
                # æ·»åŠ å…ƒæ•°æ®
                result['metadata'] = {
                    'llmLatency': time.time() - start_time,
                    'llmModel': self.llm_model,
                    'apiKeyId': api_key.key_id,
                    'retryCount': 0,
                }
                
                # å‘é€ç»“æžœ
                self._send_result(result, task.get('cameraId', 0))
                logger.info(
                    "âœ… Task {} completed in {:.2f}s, sent to Kafka",
                    task_id,
                    time.time() - start_time
                )
                
            except Exception as e:
                logger.error(f"Task {task_id} failed: {e}")
                
                # é‡Šæ”¾ Key (å¤±è´¥)
                if api_key:
                    self.key_pool.release_key(api_key, success=False)
                
                # å‘é€é”™è¯¯ç»“æžœ
                error_result = self._error_result(task, str(e))
                self._send_result(error_result, task.get('cameraId', 0))
    
    def _send_result(self, result: Dict[str, Any], camera_id: int):
        """å‘é€ç»“æžœåˆ° Kafka"""
        try:
            self.result_producer.send(result, camera_id)
            logger.info(
                "ðŸ“¤ Sent LLM result to Kafka: requestId={}, cameraId={}, risk={}",
                result.get('requestId'),
                camera_id,
                result.get('maxRiskLevel', 'none')
            )
        except Exception as e:
            logger.error(f"Failed to send result to Kafka: {e}")
    
    async def worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ï¼šæŒç»­å¤„ç†ä»»åŠ¡"""
        logger.info("Worker loop started")
        
        while self.running:
            try:
                task = await asyncio.wait_for(self.pending_tasks.get(), timeout=1.0)
                asyncio.create_task(self.process_task(task))
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Worker loop error: {e}")
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        import threading
        
        self.running = True
        
        # å¯åŠ¨ Kafka æ¶ˆè´¹çº¿ç¨‹
        consumer_thread = threading.Thread(target=self.task_consumer.start, daemon=True)
        consumer_thread.start()
        logger.info("Kafka consumer thread started")
        
        # å¯åŠ¨å¼‚æ­¥äº‹ä»¶å¾ªçŽ¯
        def run_event_loop():
            self.event_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.event_loop)
            self.event_loop.run_until_complete(self.worker_loop())
        
        loop_thread = threading.Thread(target=run_event_loop, daemon=True)
        loop_thread.start()
        logger.info("Async worker loop started")
        
        logger.success("LLM Task Scheduler started successfully")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        logger.info("Stopping LLM Task Scheduler...")
        self.running = False
        self.task_consumer.stop()
        self.result_producer.close()
        logger.info("LLM Task Scheduler stopped")
    
    def _build_prompt(self, task: Dict[str, Any]) -> str:
        """æž„å»º LLM æç¤ºè¯"""
        objects = task.get('detectedObjects', [])
        metadata = task.get('groupMetadata', {})
        
        obj_lines = []
        for obj in objects:
            bbox = obj.get('bbox', [])
            obj_lines.append(
                f"- class={obj.get('class')} confidence={obj.get('confidence', 0):.2f} bbox={bbox}"
            )
        
        objects_summary = '\n'.join(obj_lines) or 'No objects'
        
        prompt = f"""{DANGEROUS_DRIVING_PROMPT}

[Group Information]
Object count: {metadata.get('objectCount', 0)}
Object classes: {', '.join(metadata.get('classes', []))}

[Detected Objects in Group]
{objects_summary}

Please analyze this traffic scene and return results in JSON format with fields:
- riskLevel: "none" | "low" | "medium" | "high"
- confidence: 0.0-1.0
- riskTypes: array of risk types
- description: detailed description
- dangerObjectCount: number of dangerous objects
- triggerObjectIds: array of object IDs that triggered the risk
"""
        return prompt
    
    def _parse_llm_response(self, response: dict, task: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æž LLM å“åº”"""
        choices = response.get('choices', [])
        if not choices:
            return self._empty_result(task)
        
        content = choices[0].get('message', {}).get('content', '')
        
        # å°è¯•è§£æž JSON
        try:
            # æå– JSON éƒ¨åˆ†
            start = content.find('{')
            end = content.rfind('}')
            if start != -1 and end != -1:
                json_str = content[start:end + 1]
                parsed = json.loads(json_str)
            else:
                parsed = json.loads(content)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse LLM response as JSON: {content[:200]}")
            parsed = {
                'riskLevel': 'none',
                'confidence': 0.0,
                'riskTypes': [],
                'description': content[:200] if content else 'No response'
            }
        
        return {
            'messageId': f"{task.get('taskId', 'unknown')}_result",
            'requestId': task.get('requestId'),
            'cameraId': task.get('cameraId'),
            'timestamp': task.get('timestamp'),
            'results': [
                {
                    'groupIndex': task.get('groupIndex'),
                    'riskLevel': parsed.get('riskLevel', 'none'),
                    'confidence': float(parsed.get('confidence', 0.0)),
                    'riskTypes': parsed.get('riskTypes', []),
                    'description': parsed.get('description', ''),
                    'dangerObjectCount': parsed.get('dangerObjectCount'),
                    'triggerObjectIds': parsed.get('triggerObjectIds', []),
                }
            ],
            'hasDangerousDriving': parsed.get('riskLevel', 'none') != 'none',
            'maxRiskLevel': parsed.get('riskLevel', 'none'),
        }
    
    def _empty_result(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ç©ºç»“æžœ"""
        return {
            'messageId': f"{task.get('taskId', 'unknown')}_result",
            'requestId': task.get('requestId'),
            'cameraId': task.get('cameraId'),
            'timestamp': task.get('timestamp'),
            'results': [],
            'hasDangerousDriving': False,
            'maxRiskLevel': 'none',
        }
    
    def _error_result(self, task: Dict[str, Any], error: str) -> Dict[str, Any]:
        """é”™è¯¯ç»“æžœ"""
        result = self._empty_result(task)
        result['error'] = error
        result['metadata'] = {
            'llmLatency': 0.0,
            'llmModel': self.llm_model,
            'apiKeyId': None,
            'retryCount': self.max_retries,
        }
        return result
