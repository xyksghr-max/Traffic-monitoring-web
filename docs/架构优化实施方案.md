# å®æ—¶å±é™©ç”»é¢æ£€æµ‹ç³»ç»Ÿå¹¶å‘å¤„ç†èƒ½åŠ›ä¼˜åŒ– - æŠ€æœ¯å®æ–½æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•
- [1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡](#1-é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡)
- [2. ç°æœ‰ç³»ç»Ÿåˆ†æ](#2-ç°æœ‰ç³»ç»Ÿåˆ†æ)
- [3. ç›®æ ‡æ¶æ„è®¾è®¡](#3-ç›®æ ‡æ¶æ„è®¾è®¡)
- [4. è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ](#4-è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ)
- [5. å®æ–½è®¡åˆ’](#5-å®æ–½è®¡åˆ’)
- [6. é£é™©è¯„ä¼°ä¸åº”å¯¹](#6-é£é™©è¯„ä¼°ä¸åº”å¯¹)
- [7. éªŒæ”¶æ ‡å‡†](#7-éªŒæ”¶æ ‡å‡†)

---

## 1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 å½“å‰çŠ¶å†µåˆ†æ

**ç°æœ‰æ¶æ„æµç¨‹ï¼š**
```
æ‘„åƒå¤´è§†é¢‘æµ -> YOLOç›®æ ‡æ£€æµ‹ -> BXèšç±»(GroupAnalyzer) -> 
å¤šæ¨¡æ€å¤§æ¨¡å‹(DangerousDrivingAnalyzer) -> WebSocketæ¨é€ç»“æœ
```

**æ ¸å¿ƒç“¶é¢ˆï¼š**
1. **å•API Keyé™åˆ¶**ï¼š`DangerousDrivingAnalyzer` ä»…ä½¿ç”¨å•ä¸ª `DASHSCOPE_API_KEY`
2. **åŒæ­¥é˜»å¡è°ƒç”¨**ï¼šåœ¨ `DetectionPipeline._run()` ä¸»å¾ªç¯ä¸­ç›´æ¥è°ƒç”¨LLMï¼Œé˜»å¡æ•´ä¸ªå¤„ç†æµç¨‹
3. **QPS/RPMä¸Šé™**ï¼šå•Keyæ— æ³•æ”¯æŒå¤šè·¯æ‘„åƒå¤´é«˜å¹¶å‘åœºæ™¯
4. **ç¼ºä¹å¼¹æ€§æ‰©å±•**ï¼šæ— æ³•é€šè¿‡å¢åŠ API Keyæ¥çº¿æ€§æå‡å¤„ç†èƒ½åŠ›

**é‡åŒ–æŒ‡æ ‡ï¼š**
- å½“å‰å•Key QPSçº¦: 5-10æ¬¡/ç§’
- å•æ¬¡LLMè°ƒç”¨è€—æ—¶: 1-3ç§’
- æœŸæœ›æ”¯æŒæ‘„åƒå¤´æ•°: 50+è·¯
- æœŸæœ›ç«¯åˆ°ç«¯å»¶è¿Ÿ: <2ç§’

### 1.2 ä¼˜åŒ–ç›®æ ‡

#### ä¸šåŠ¡ç›®æ ‡
1. **å¹¶å‘èƒ½åŠ›æå‡10å€ä»¥ä¸Š**ï¼šä»å•è·¯æ”¯æŒæå‡è‡³50+è·¯åŒæ—¶å¤„ç†
2. **é™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿ**ï¼šä»3-5ç§’é™ä½åˆ°2ç§’ä»¥å†…
3. **æé«˜ç³»ç»Ÿç¨³å®šæ€§**ï¼šå•ç‚¹æ•…éšœä¸å½±å“æ•´ä½“æœåŠ¡
4. **æ”¯æŒå¼¹æ€§æ‰©å±•**ï¼šå¯é€šè¿‡å¢åŠ Key/å®ä¾‹æ°´å¹³æ‰©å±•

#### æŠ€æœ¯ç›®æ ‡
1. å¼•å…¥æ¶ˆæ¯é˜Ÿåˆ—(Kafka)å®ç°è§£è€¦
2. å¼•å…¥æµå¤„ç†å¼•æ“(Flink)å®ç°ä»»åŠ¡ç¼–æ’
3. å®ç°å¤šAPI Keyæ± åŒ–è°ƒåº¦
4. æ„å»ºå¯è§‚æµ‹çš„ç›‘æ§ä½“ç³»

---

## 2. ç°æœ‰ç³»ç»Ÿåˆ†æ

### 2.1 ä»£ç ç»“æ„åˆ†æ

**æ ¸å¿ƒæ¨¡å—ï¼š**
```
algo/
â”œâ”€â”€ rtsp_detect/
â”‚   â”œâ”€â”€ pipeline.py              # æ£€æµ‹æµæ°´çº¿(ä¸»å¾ªç¯)
â”‚   â”œâ”€â”€ yolo_detector.py         # YOLOæ¨ç†
â”‚   â”œâ”€â”€ group_analyzer.py        # ç¾¤ç»„èšç±»(BXç®—æ³•)
â”‚   â”œâ”€â”€ video_stream.py          # RTSPæ‹‰æµ
â”‚   â””â”€â”€ risk_alert_manager.py    # å‘Šè­¦ç®¡ç†
â””â”€â”€ llm/
    â”œâ”€â”€ dangerous_driving_detector.py  # LLMè°ƒç”¨å°è£… âš ï¸ ç“¶é¢ˆç‚¹
    â””â”€â”€ prompts.py               # Promptæ¨¡æ¿
```

**å…³é”®é—®é¢˜å®šä½ï¼š**

**`DetectionPipeline._run()` ä¸»å¾ªç¯ï¼š**
```python
def _run(self) -> None:
    while not self._stop_event.is_set():
        frame = self.stream.get_latest_frame()
        detection = self.detector.detect(frame)              # ~200ms
        groups, group_images = self._analyze_groups(...)     # ~50ms
        llm_result = self._analyze_dangerous_driving(...)    # ğŸ”´ 1-3ç§’ï¼Œé˜»å¡æ•´ä¸ªå¾ªç¯
        self.callback(payload)                               # WebSocketæ¨é€
        time.sleep(self.frame_interval)                      # 1.8ç§’
```

**`DangerousDrivingAnalyzer.analyze()` åŒæ­¥è°ƒç”¨ï¼š**
```python
def analyze(self, ...) -> Dict[str, Any]:
    # å•Keyè°ƒç”¨ï¼Œæ— å¹¶å‘æ§åˆ¶
    response = self._client.chat.completions.create(
        model=self.config.model,
        messages=messages,
        timeout=self.config.timeout,  # 30ç§’è¶…æ—¶
    )
```

### 2.2 æ€§èƒ½ç“¶é¢ˆé‡åŒ–

| é˜¶æ®µ | å¹³å‡è€—æ—¶ | ç“¶é¢ˆç‚¹ |
|------|---------|--------|
| è§†é¢‘æ‹‰æµ | æŒç»­ | RTSPç½‘ç»œ |
| YOLOæ¨ç† | 200ms | GPU/CPU |
| BXèšç±» | 50ms | CPU |
| **LLMè°ƒç”¨** | **1-3ç§’** | **API QPSé™åˆ¶** âš ï¸ |
| ç»“æœç¼–ç  | 100ms | CPU |
| WebSocketæ¨é€ | 10ms | ç½‘ç»œ |

**ç»“è®ºï¼š** LLMè°ƒç”¨å æ€»å¤„ç†æ—¶é—´çš„60-80%ï¼Œä¸”å—å•Key QPSé™åˆ¶ï¼Œæ˜¯ç³»ç»Ÿæœ€å¤§ç“¶é¢ˆã€‚

---

## 3. ç›®æ ‡æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ‘„åƒå¤´ç¾¤ç»„  â”‚â”€â”€â”€â”€â–¶â”‚ YOLO+BXèšç±»  â”‚â”€â”€â”€â”€â–¶â”‚ Kafka Producer  â”‚
â”‚  (Nè·¯è§†é¢‘)  â”‚     â”‚  æ£€æµ‹æœåŠ¡    â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚  Kafka Cluster   â”‚
                                         â”‚ detection-resultsâ”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                                               â”‚
                          â–¼                                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Flink Consumer   â”‚                          â”‚ ç›´æ¥æ¶ˆè´¹(å¯é€‰)   â”‚
                â”‚  - æ•°æ®éªŒè¯        â”‚                          â”‚ å¦‚ï¼šå®æ—¶å¤§å±     â”‚
                â”‚  - æ ¼å¼è½¬æ¢        â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚  - ä»»åŠ¡ç”Ÿæˆ        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  ä»»åŠ¡è°ƒåº¦å™¨ Pool   â”‚
                â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                â”‚ â”‚ API Key Pool  â”‚ â”‚â—€â”€â”€ Key1, Key2, Key3, ... KeyN
                â”‚ â”‚ - è½®è¯¢è°ƒåº¦     â”‚ â”‚
                â”‚ â”‚ - å¥åº·æ£€æŸ¥     â”‚ â”‚
                â”‚ â”‚ - é™æµæ§åˆ¶     â”‚ â”‚
                â”‚ â”‚ - å¤±è´¥é‡è¯•     â”‚ â”‚
                â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ (å¹¶å‘è°ƒç”¨LLM)
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  å¤šæ¨¡æ€å¤§æ¨¡å‹ API  â”‚
                â”‚  (å¹¶å‘N*QPS)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Kafka Producer    â”‚
                â”‚ risk-assessment-  â”‚
                â”‚ results-topic     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ ä¸‹æ¸¸æ¶ˆè´¹æœåŠ¡       â”‚
                â”‚ - å‘Šè­¦æœåŠ¡         â”‚
                â”‚ - æ•°æ®å­˜å‚¨         â”‚
                â”‚ - WebSocketæ¨é€    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æµè®¾è®¡

#### æµè½¬é˜¶æ®µï¼š

**é˜¶æ®µ1: æ£€æµ‹é˜¶æ®µ**
```
Video Frame â†’ YOLO â†’ BX Clustering â†’ DetectionResult
```

**é˜¶æ®µ2: æ¶ˆæ¯å‘å¸ƒ**
```
DetectionResult â†’ JSON Serialization â†’ Kafka(detection-results-topic)
```

**é˜¶æ®µ3: ä»»åŠ¡åˆ†å‘**
```
Kafka â†’ Flink Consumer â†’ Task Generation â†’ AssessmentTask
```

**é˜¶æ®µ4: å¹¶å‘è¯„ä¼°**
```
AssessmentTask â†’ Scheduler â†’ API Key Pool â†’ LLM API (å¹¶å‘è°ƒç”¨)
```

**é˜¶æ®µ5: ç»“æœæ±‡æ€»**
```
LLM Response â†’ Result Aggregation â†’ Kafka(risk-assessment-results-topic)
```

**é˜¶æ®µ6: ä¸šåŠ¡æ¶ˆè´¹**
```
Kafka â†’ Consumer Services â†’ [Alert/Storage/WebSocket]
```

### 3.3 å…³é”®æŠ€æœ¯é€‰å‹

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç†ç”± |
|-----|---------|-----|
| æ¶ˆæ¯é˜Ÿåˆ— | **Kafka** | é«˜ååã€æŒä¹…åŒ–ã€åˆ†åŒºæ”¯æŒã€ç”Ÿæ€æˆç†Ÿ |
| æµå¤„ç† | **Flink (Python)** | å®æ—¶å¤„ç†ã€çŠ¶æ€ç®¡ç†ã€exactly-onceè¯­ä¹‰ |
| APIè°ƒåº¦ | **è‡ªç ”Scheduler** | çµæ´»æ§åˆ¶ã€ä¸šåŠ¡å®šåˆ¶ã€è½»é‡çº§ |
| ä»»åŠ¡é˜Ÿåˆ—(å¤‡é€‰) | **Celery+Redis** | åˆ†å¸ƒå¼ä»»åŠ¡ã€é‡è¯•æœºåˆ¶ã€ç›‘æ§ |
| ç›‘æ§ | **Prometheus+Grafana** | æŒ‡æ ‡é‡‡é›†ã€å¯è§†åŒ–ã€å‘Šè­¦ |
| æ—¥å¿— | **ELK Stack** | é›†ä¸­å¼æ—¥å¿—ã€å…¨æ–‡æœç´¢ã€åˆ†æ |

---

## 4. è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ

### 4.1 Kafkaé›†æˆæ–¹æ¡ˆ

#### 4.1.1 Topicè®¾è®¡

**Topic 1: `detection-results`**
- **ç”¨é€”**: å­˜å‚¨YOLO+BXæ£€æµ‹ç»“æœ
- **åˆ†åŒºæ•°**: 16 (æ ¹æ®cameraIdå“ˆå¸Œåˆ†åŒº)
- **å‰¯æœ¬æ•°**: 3
- **ä¿ç•™æ—¶é•¿**: 1å°æ—¶
- **å‹ç¼©**: Snappy

**æ¶ˆæ¯Schema (JSON):**
```json
{
  "messageId": "uuid-v4",
  "cameraId": 1,
  "timestamp": "2025-10-20T10:30:45.123Z",
  "imageWidth": 1920,
  "imageHeight": 1080,
  "rawFrameUrl": "s3://bucket/frames/camera1_20251020_103045.jpg",
  "detectedObjects": [
    {
      "objectId": 1,
      "class": "car",
      "confidence": 0.92,
      "bbox": [120, 200, 360, 520]
    }
  ],
  "trafficGroups": [
    {
      "groupIndex": 1,
      "objectCount": 3,
      "bbox": [100, 180, 400, 550],
      "classes": ["car", "person"],
      "groupImageUrl": "s3://bucket/groups/group1_xxx.jpg",
      "groupImageBase64": "data:image/jpeg;base64,/9j/4AAQ..."
    }
  ],
  "metadata": {
    "detectionLatency": 0.25,
    "yoloModel": "yolov8n",
    "processingNode": "node-01"
  }
}
```

**Topic 2: `risk-assessment-results`**
- **ç”¨é€”**: å­˜å‚¨LLMé£é™©è¯„ä¼°ç»“æœ
- **åˆ†åŒºæ•°**: 16
- **å‰¯æœ¬æ•°**: 3
- **ä¿ç•™æ—¶é•¿**: 24å°æ—¶

**æ¶ˆæ¯Schema (JSON):**
```json
{
  "messageId": "uuid-v4",
  "requestId": "å…³è”detection-resultsçš„messageId",
  "cameraId": 1,
  "timestamp": "2025-10-20T10:30:47.456Z",
  "results": [
    {
      "groupIndex": 1,
      "riskLevel": "high",
      "confidence": 0.88,
      "riskTypes": ["tailgating", "speeding"],
      "description": "ä¸¤è½¦è·ç¦»è¿‡è¿‘ï¼Œå­˜åœ¨è¿½å°¾é£é™©",
      "triggerObjectIds": [1, 2],
      "dangerObjectCount": 2
    }
  ],
  "hasDangerousDriving": true,
  "maxRiskLevel": "high",
  "metadata": {
    "llmLatency": 1.35,
    "llmModel": "qwen-vl-plus",
    "apiKey": "key_hash_xxx",
    "retryCount": 0,
    "processingNode": "scheduler-02"
  }
}
```

#### 4.1.2 Producerå®ç°

**æ–‡ä»¶: `algo/kafka/detection_producer.py`**
```python
from confluent_kafka import Producer
from typing import Dict, Any
import json
import uuid
from loguru import logger

class DetectionResultProducer:
    def __init__(self, bootstrap_servers: str, topic: str):
        self.topic = topic
        self.producer = Producer({
            'bootstrap.servers': bootstrap_servers,
            'compression.type': 'snappy',
            'linger.ms': 10,  # æ‰¹å¤„ç†å»¶è¿Ÿ
            'batch.size': 32768,
            'acks': 1,  # å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§
        })
    
    def send(self, detection_result: Dict[str, Any]) -> None:
        """å‘é€æ£€æµ‹ç»“æœåˆ°Kafka"""
        message_id = str(uuid.uuid4())
        detection_result['messageId'] = message_id
        
        key = str(detection_result['cameraId']).encode('utf-8')
        value = json.dumps(detection_result).encode('utf-8')
        
        try:
            self.producer.produce(
                topic=self.topic,
                key=key,
                value=value,
                callback=self._delivery_callback
            )
            self.producer.poll(0)  # éé˜»å¡
        except Exception as e:
            logger.error(f"Failed to produce message: {e}")
    
    def _delivery_callback(self, err, msg):
        if err:
            logger.error(f"Message delivery failed: {err}")
        else:
            logger.debug(f"Message delivered to {msg.topic()}[{msg.partition()}]")
    
    def flush(self):
        """ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ"""
        self.producer.flush()
```

**é›†æˆåˆ° `DetectionPipeline`:**
```python
# ä¿®æ”¹ algo/rtsp_detect/pipeline.py

class DetectionPipeline:
    def __init__(self, ..., kafka_producer: Optional[DetectionResultProducer] = None):
        self.kafka_producer = kafka_producer
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
    
    def _run(self) -> None:
        while not self._stop_event.is_set():
            frame = self.stream.get_latest_frame()
            if frame is None:
                time.sleep(0.1)
                continue
            
            # YOLOæ£€æµ‹
            detection = self.detector.detect(frame)
            detected_objects = detection.get("objects", [])
            
            # BXèšç±»
            groups, group_images = self._analyze_groups(raw_frame, detected_objects)
            
            # ğŸ”¥ æ–°å¢: å‘é€åˆ°Kafkaè€Œéç›´æ¥è°ƒç”¨LLM
            if self.kafka_producer:
                detection_payload = self._build_detection_payload(
                    frame, detected_objects, groups, group_images
                )
                self.kafka_producer.send(detection_payload)
            
            # ğŸ”¥ ç§»é™¤åŸæœ‰çš„åŒæ­¥LLMè°ƒç”¨
            # llm_result = self._analyze_dangerous_driving(...)
            
            # ç»§ç»­å…¶ä»–å¤„ç†...
            time.sleep(self.frame_interval)
```

#### 4.1.3 Consumerå®ç°åŸºç¡€

**æ–‡ä»¶: `algo/kafka/base_consumer.py`**
```python
from confluent_kafka import Consumer, KafkaError
from typing import Callable, Dict, Any
import json
from loguru import logger

class BaseKafkaConsumer:
    def __init__(
        self, 
        bootstrap_servers: str,
        group_id: str,
        topics: list[str],
        message_handler: Callable[[Dict[str, Any]], None]
    ):
        self.topics = topics
        self.message_handler = message_handler
        self.consumer = Consumer({
            'bootstrap.servers': bootstrap_servers,
            'group.id': group_id,
            'auto.offset.reset': 'latest',
            'enable.auto.commit': True,
            'auto.commit.interval.ms': 5000,
        })
        self.running = False
    
    def start(self):
        """å¯åŠ¨æ¶ˆè´¹è€…"""
        self.consumer.subscribe(self.topics)
        self.running = True
        logger.info(f"Kafka consumer started for topics: {self.topics}")
        
        try:
            while self.running:
                msg = self.consumer.poll(timeout=1.0)
                if msg is None:
                    continue
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        logger.error(f"Consumer error: {msg.error()}")
                        continue
                
                try:
                    data = json.loads(msg.value().decode('utf-8'))
                    self.message_handler(data)
                except Exception as e:
                    logger.error(f"Failed to process message: {e}")
        finally:
            self.consumer.close()
    
    def stop(self):
        """åœæ­¢æ¶ˆè´¹è€…"""
        self.running = False
```

### 4.2 Flinkæµå¤„ç†æ–¹æ¡ˆ

#### 4.2.1 Flinkä½œä¸šè®¾è®¡

**ä¸ºä»€ä¹ˆé€‰æ‹©PyFlinkï¼š**
- ä¸ç°æœ‰PythonæŠ€æœ¯æ ˆç»Ÿä¸€
- æ”¯æŒKafkaè¿æ¥å™¨
- æä¾›çª—å£èšåˆã€çŠ¶æ€ç®¡ç†ç­‰æµå¤„ç†èƒ½åŠ›

**æ–‡ä»¶: `flink_jobs/assessment_task_generator.py`**
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import (
    KafkaSource, KafkaOffsetsInitializer, KafkaSink, KafkaRecordSerializationSchema
)
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
import json
from loguru import logger

class AssessmentTaskGenerator:
    """Flinkä½œä¸šï¼šæ¶ˆè´¹æ£€æµ‹ç»“æœï¼Œç”Ÿæˆè¯„ä¼°ä»»åŠ¡"""
    
    def __init__(self, kafka_bootstrap: str, input_topic: str, output_topic: str):
        self.kafka_bootstrap = kafka_bootstrap
        self.input_topic = input_topic
        self.output_topic = output_topic
        self.env = StreamExecutionEnvironment.get_execution_environment()
        
        # è®¾ç½®checkpoint
        self.env.enable_checkpointing(10000)  # 10ç§’
        self.env.get_checkpoint_config().set_checkpoint_storage_dir("file:///tmp/flink-checkpoints")
    
    def process_detection_result(self, data_str: str) -> list:
        """
        å¤„ç†æ£€æµ‹ç»“æœï¼Œä¸ºæ¯ä¸ªgroupç”Ÿæˆè¯„ä¼°ä»»åŠ¡
        è¿”å›: [AssessmentTask, ...]
        """
        try:
            data = json.loads(data_str)
            tasks = []
            
            for group in data.get('trafficGroups', []):
                if group.get('objectCount', 0) < 2:
                    continue  # è·³è¿‡å•ç›®æ ‡ç»„
                
                task = {
                    'taskId': f"{data['messageId']}_group{group['groupIndex']}",
                    'requestId': data['messageId'],
                    'cameraId': data['cameraId'],
                    'timestamp': data['timestamp'],
                    'groupIndex': group['groupIndex'],
                    'groupImageBase64': group.get('groupImageBase64', ''),
                    'groupImageUrl': group.get('groupImageUrl', ''),
                    'detectedObjects': [
                        obj for obj in data.get('detectedObjects', [])
                        if self._is_in_group(obj, group)
                    ],
                    'groupMetadata': {
                        'objectCount': group['objectCount'],
                        'classes': group['classes'],
                        'bbox': group['bbox'],
                    }
                }
                tasks.append(json.dumps(task))
            
            return tasks
        except Exception as e:
            logger.error(f"Failed to process detection result: {e}")
            return []
    
    def _is_in_group(self, obj: dict, group: dict) -> bool:
        """åˆ¤æ–­ç›®æ ‡æ˜¯å¦åœ¨ç¾¤ç»„å†…"""
        obj_bbox = obj['bbox']
        group_bbox = group['bbox']
        # ç®€å•åˆ¤æ–­ï¼šç›®æ ‡ä¸­å¿ƒæ˜¯å¦åœ¨ç¾¤ç»„bboxå†…
        obj_cx = (obj_bbox[0] + obj_bbox[2]) / 2
        obj_cy = (obj_bbox[1] + obj_bbox[3]) / 2
        return (group_bbox[0] <= obj_cx <= group_bbox[2] and 
                group_bbox[1] <= obj_cy <= group_bbox[3])
    
    def run(self):
        """è¿è¡ŒFlinkä½œä¸š"""
        # é…ç½®Kafka Source
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(self.kafka_bootstrap) \
            .set_topics(self.input_topic) \
            .set_group_id("flink-assessment-task-generator") \
            .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()
        
        # è¯»å–æ•°æ®æµ
        detection_stream = self.env.from_source(
            kafka_source,
            watermark_strategy=None,
            source_name="Kafka Detection Results"
        )
        
        # å¤„ç†å¹¶ç”Ÿæˆä»»åŠ¡
        task_stream = detection_stream.flat_map(
            self.process_detection_result,
            output_type=Types.STRING()
        )
        
        # é…ç½®Kafka Sink (å‘é€åˆ°ä»»åŠ¡é˜Ÿåˆ—topic)
        kafka_sink = KafkaSink.builder() \
            .set_bootstrap_servers(self.kafka_bootstrap) \
            .set_record_serializer(
                KafkaRecordSerializationSchema.builder()
                    .set_topic("assessment-tasks")
                    .set_value_serialization_schema(SimpleStringSchema())
                    .build()
            ) \
            .build()
        
        task_stream.sink_to(kafka_sink)
        
        # æ‰§è¡Œä½œä¸š
        self.env.execute("Assessment Task Generator Job")

if __name__ == "__main__":
    generator = AssessmentTaskGenerator(
        kafka_bootstrap="localhost:9092",
        input_topic="detection-results",
        output_topic="assessment-tasks"
    )
    generator.run()
```

#### 4.2.2 å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥Pythonæ¶ˆè´¹

**å¦‚æœä¸å¸Œæœ›å¼•å…¥Flinkçš„å¤æ‚åº¦ï¼Œå¯ä»¥ä½¿ç”¨çº¯Pythonå®ç°ï¼š**

**æ–‡ä»¶: `algo/task_generator/simple_generator.py`**
```python
import threading
from algo.kafka.base_consumer import BaseKafkaConsumer
from algo.kafka.detection_producer import DetectionResultProducer
from typing import Dict, Any
from loguru import logger

class SimpleTaskGenerator:
    """ç®€åŒ–ç‰ˆä»»åŠ¡ç”Ÿæˆå™¨ (ä¸ä½¿ç”¨Flink)"""
    
    def __init__(self, kafka_bootstrap: str):
        self.consumer = BaseKafkaConsumer(
            bootstrap_servers=kafka_bootstrap,
            group_id="simple-task-generator",
            topics=["detection-results"],
            message_handler=self.handle_detection_result
        )
        self.task_producer = DetectionResultProducer(
            bootstrap_servers=kafka_bootstrap,
            topic="assessment-tasks"
        )
    
    def handle_detection_result(self, data: Dict[str, Any]):
        """å¤„ç†æ£€æµ‹ç»“æœå¹¶ç”Ÿæˆä»»åŠ¡"""
        try:
            for group in data.get('trafficGroups', []):
                if group.get('objectCount', 0) < 2:
                    continue
                
                task = {
                    'taskId': f"{data['messageId']}_group{group['groupIndex']}",
                    'requestId': data['messageId'],
                    'cameraId': data['cameraId'],
                    'groupIndex': group['groupIndex'],
                    'groupImageBase64': group.get('groupImageBase64', ''),
                    'detectedObjects': data.get('detectedObjects', []),
                    'groupMetadata': group,
                }
                self.task_producer.send(task)
        except Exception as e:
            logger.error(f"Task generation error: {e}")
    
    def start(self):
        thread = threading.Thread(target=self.consumer.start, daemon=True)
        thread.start()
        logger.info("Simple task generator started")
```

### 4.3 API Keyæ± åŒ–è°ƒåº¦å™¨

#### 4.3.1 æ ¸å¿ƒè®¾è®¡

**æ–‡ä»¶: `algo/scheduler/api_key_pool.py`**
```python
import time
import threading
from dataclasses import dataclass
from enum import Enum
from typing import List, Optional
from collections import deque
from loguru import logger

class KeyStatus(Enum):
    AVAILABLE = "available"
    IN_USE = "in_use"
    COOLING = "cooling"
    DISABLED = "disabled"

@dataclass
class APIKey:
    key: str
    key_id: str
    status: KeyStatus = KeyStatus.AVAILABLE
    total_calls: int = 0
    success_calls: int = 0
    failed_calls: int = 0
    last_used_at: float = 0.0
    cooldown_until: float = 0.0
    qps_limit: int = 10  # å•ä¸ªKeyçš„QPSé™åˆ¶
    rpm_limit: int = 300  # å•ä¸ªKeyçš„RPMé™åˆ¶

class APIKeyPool:
    """API Keyæ± ç®¡ç†å™¨"""
    
    def __init__(self, keys: List[str], cooldown_seconds: float = 60.0):
        self.keys = [
            APIKey(key=key, key_id=f"key_{i}") 
            for i, key in enumerate(keys, 1)
        ]
        self.cooldown_seconds = cooldown_seconds
        self.lock = threading.RLock()
        self.call_history = deque(maxlen=1000)  # è®°å½•æœ€è¿‘1000æ¬¡è°ƒç”¨
        
        logger.info(f"Initialized API Key Pool with {len(self.keys)} keys")
    
    def acquire_key(self, timeout: float = 5.0) -> Optional[APIKey]:
        """
        è·å–ä¸€ä¸ªå¯ç”¨çš„API Key
        ç­–ç•¥: è½®è¯¢ + è´Ÿè½½å‡è¡¡ (é€‰æ‹©è°ƒç”¨æ¬¡æ•°æœ€å°‘çš„)
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            with self.lock:
                available_keys = [
                    k for k in self.keys
                    if k.status == KeyStatus.AVAILABLE and 
                    time.time() >= k.cooldown_until
                ]
                
                if not available_keys:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å†·å´å®Œæˆçš„Key
                    cooling_keys = [k for k in self.keys if k.status == KeyStatus.COOLING]
                    for key in cooling_keys:
                        if time.time() >= key.cooldown_until:
                            key.status = KeyStatus.AVAILABLE
                            available_keys.append(key)
                
                if available_keys:
                    # é€‰æ‹©è°ƒç”¨æ¬¡æ•°æœ€å°‘çš„Key (è´Ÿè½½å‡è¡¡)
                    selected_key = min(available_keys, key=lambda k: k.total_calls)
                    selected_key.status = KeyStatus.IN_USE
                    selected_key.last_used_at = time.time()
                    return selected_key
            
            time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
        
        logger.warning("Failed to acquire API key within timeout")
        return None
    
    def release_key(self, key: APIKey, success: bool = True):
        """é‡Šæ”¾API Key"""
        with self.lock:
            key.total_calls += 1
            if success:
                key.success_calls += 1
                key.status = KeyStatus.AVAILABLE
            else:
                key.failed_calls += 1
                # å¤±è´¥åˆ™è¿›å…¥å†·å´æœŸ
                key.status = KeyStatus.COOLING
                key.cooldown_until = time.time() + self.cooldown_seconds
                logger.warning(f"API Key {key.key_id} entered cooling period")
            
            self.call_history.append({
                'key_id': key.key_id,
                'timestamp': time.time(),
                'success': success
            })
    
    def get_stats(self) -> dict:
        """è·å–æ± ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'total_keys': len(self.keys),
                'available_keys': sum(1 for k in self.keys if k.status == KeyStatus.AVAILABLE),
                'in_use_keys': sum(1 for k in self.keys if k.status == KeyStatus.IN_USE),
                'cooling_keys': sum(1 for k in self.keys if k.status == KeyStatus.COOLING),
                'disabled_keys': sum(1 for k in self.keys if k.status == KeyStatus.DISABLED),
                'total_calls': sum(k.total_calls for k in self.keys),
                'success_rate': sum(k.success_calls for k in self.keys) / max(sum(k.total_calls for k in self.keys), 1),
                'keys': [
                    {
                        'key_id': k.key_id,
                        'status': k.status.value,
                        'total_calls': k.total_calls,
                        'success_rate': k.success_calls / max(k.total_calls, 1)
                    }
                    for k in self.keys
                ]
            }
```

#### 4.3.2 ä»»åŠ¡è°ƒåº¦å™¨å®ç°

**æ–‡ä»¶: `algo/scheduler/task_scheduler.py`**
```python
import asyncio
import aiohttp
from typing import Dict, Any, List
from loguru import logger
from algo.scheduler.api_key_pool import APIKeyPool, APIKey
from algo.kafka.base_consumer import BaseKafkaConsumer
from algo.kafka.detection_producer import DetectionResultProducer
import json

class LLMTaskScheduler:
    """LLMä»»åŠ¡è°ƒåº¦å™¨ - æ”¯æŒå¤šKeyå¹¶å‘è°ƒç”¨"""
    
    def __init__(
        self,
        key_pool: APIKeyPool,
        kafka_bootstrap: str,
        max_concurrent_tasks: int = 50
    ):
        self.key_pool = key_pool
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        
        # Kafkaæ¶ˆè´¹è€…å’Œç”Ÿäº§è€…
        self.task_consumer = BaseKafkaConsumer(
            bootstrap_servers=kafka_bootstrap,
            group_id="llm-task-scheduler",
            topics=["assessment-tasks"],
            message_handler=self.handle_task
        )
        self.result_producer = DetectionResultProducer(
            bootstrap_servers=kafka_bootstrap,
            topic="risk-assessment-results"
        )
        
        self.running = False
        self.pending_tasks = asyncio.Queue()
    
    def handle_task(self, task_data: Dict[str, Any]):
        """æ¥æ”¶ä»»åŠ¡å¹¶æ”¾å…¥å¼‚æ­¥é˜Ÿåˆ—"""
        asyncio.create_task(self.pending_tasks.put(task_data))
    
    async def call_llm_api(
        self, 
        api_key: APIKey, 
        task: Dict[str, Any],
        max_retries: int = 2
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šçš„API Keyè°ƒç”¨å¤§æ¨¡å‹API
        """
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        headers = {
            "Authorization": f"Bearer {api_key.key}",
            "Content-Type": "application/json"
        }
        
        # æ„é€ è¯·æ±‚payload
        group_image = task.get('groupImageBase64', '')
        prompt = self._build_prompt(task)
        
        payload = {
            "model": "qwen-vl-plus",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": group_image}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ],
            "timeout": 30
        }
        
        for attempt in range(max_retries + 1):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{base_url}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=35)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            return self._parse_llm_response(result, task)
                        elif response.status == 429:  # Rate limit
                            logger.warning(f"Rate limit hit for {api_key.key_id}, attempt {attempt + 1}")
                            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        else:
                            error_text = await response.text()
                            logger.error(f"LLM API error {response.status}: {error_text}")
                            if attempt == max_retries:
                                raise Exception(f"API call failed after {max_retries} retries")
            except asyncio.TimeoutError:
                logger.error(f"LLM API timeout for {api_key.key_id}, attempt {attempt + 1}")
                if attempt == max_retries:
                    raise
            except Exception as e:
                logger.error(f"LLM API exception: {e}")
                if attempt == max_retries:
                    raise
        
        return self._empty_result(task)
    
    async def process_task(self, task: Dict[str, Any]):
        """å¤„ç†å•ä¸ªè¯„ä¼°ä»»åŠ¡"""
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            api_key = self.key_pool.acquire_key(timeout=10.0)
            if not api_key:
                logger.error(f"Failed to acquire API key for task {task.get('taskId')}")
                return
            
            try:
                result = await self.call_llm_api(api_key, task)
                self.key_pool.release_key(api_key, success=True)
                
                # å‘é€ç»“æœåˆ°Kafka
                self.result_producer.send(result)
                logger.info(f"Task {task.get('taskId')} completed successfully")
            except Exception as e:
                logger.error(f"Task {task.get('taskId')} failed: {e}")
                self.key_pool.release_key(api_key, success=False)
                
                # å‘é€å¤±è´¥ç»“æœ
                error_result = self._error_result(task, str(e))
                self.result_producer.send(error_result)
    
    async def worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ï¼šæŒç»­å¤„ç†ä»»åŠ¡"""
        while self.running:
            try:
                task = await asyncio.wait_for(self.pending_tasks.get(), timeout=1.0)
                asyncio.create_task(self.process_task(task))
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Worker loop error: {e}")
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.running = True
        
        # å¯åŠ¨Kafkaæ¶ˆè´¹çº¿ç¨‹
        import threading
        consumer_thread = threading.Thread(target=self.task_consumer.start, daemon=True)
        consumer_thread.start()
        
        # å¯åŠ¨å¼‚æ­¥å·¥ä½œå¾ªç¯
        loop = asyncio.get_event_loop()
        asyncio.ensure_future(self.worker_loop())
        logger.info("LLM Task Scheduler started")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        self.task_consumer.stop()
        logger.info("LLM Task Scheduler stopped")
    
    def _build_prompt(self, task: Dict[str, Any]) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        objects = task.get('detectedObjects', [])
        metadata = task.get('groupMetadata', {})
        
        prompt = f"""
        è¯·åˆ†æè¿™å¼ äº¤é€šåœºæ™¯å›¾ç‰‡ä¸­çš„å±é™©é©¾é©¶è¡Œä¸ºã€‚
        
        æ£€æµ‹åˆ°çš„å¯¹è±¡æ•°é‡: {metadata.get('objectCount', 0)}
        å¯¹è±¡ç±»åˆ«: {', '.join(metadata.get('classes', []))}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - riskLevel: "none" | "low" | "medium" | "high"
        - confidence: 0.0-1.0
        - riskTypes: [é£é™©ç±»å‹æ•°ç»„]
        - description: è¯¦ç»†æè¿°
        """
        return prompt
    
    def _parse_llm_response(self, response: dict, task: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        choices = response.get('choices', [])
        if not choices:
            return self._empty_result(task)
        
        content = choices[0].get('message', {}).get('content', '')
        
        # è§£æJSONå“åº” (å®é™…éœ€è¦æ›´robustçš„è§£æé€»è¾‘)
        try:
            parsed = json.loads(content)
        except:
            parsed = {'riskLevel': 'none', 'confidence': 0.0, 'riskTypes': [], 'description': ''}
        
        return {
            'messageId': f"{task['taskId']}_result",
            'requestId': task['requestId'],
            'cameraId': task['cameraId'],
            'timestamp': task['timestamp'],
            'results': [
                {
                    'groupIndex': task['groupIndex'],
                    'riskLevel': parsed.get('riskLevel', 'none'),
                    'confidence': parsed.get('confidence', 0.0),
                    'riskTypes': parsed.get('riskTypes', []),
                    'description': parsed.get('description', ''),
                }
            ],
            'hasDangerousDriving': parsed.get('riskLevel', 'none') != 'none',
            'maxRiskLevel': parsed.get('riskLevel', 'none'),
        }
    
    def _empty_result(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ç©ºç»“æœ"""
        return {
            'messageId': f"{task['taskId']}_result",
            'requestId': task['requestId'],
            'cameraId': task['cameraId'],
            'results': [],
            'hasDangerousDriving': False,
            'maxRiskLevel': 'none',
        }
    
    def _error_result(self, task: Dict[str, Any], error: str) -> Dict[str, Any]:
        """é”™è¯¯ç»“æœ"""
        result = self._empty_result(task)
        result['error'] = error
        return result
```

### 4.4 ä¸‹æ¸¸æ¶ˆè´¹æœåŠ¡

#### 4.4.1 ç»“æœèšåˆæ¶ˆè´¹è€…

**æ–‡ä»¶: `algo/consumers/result_aggregator.py`**
```python
from algo.kafka.base_consumer import BaseKafkaConsumer
from typing import Dict, Any
from loguru import logger
import redis
import json

class ResultAggregator:
    """
    ç»“æœèšåˆå™¨ï¼š
    1. å°†LLMç»“æœä¸åŸå§‹æ£€æµ‹ç»“æœå…³è”
    2. ç¼“å­˜åˆ°Redisä¾›WebSocketå¿«é€ŸæŸ¥è¯¢
    3. è§¦å‘å‘Šè­¦é€»è¾‘
    """
    
    def __init__(self, kafka_bootstrap: str, redis_client: redis.Redis):
        self.redis = redis_client
        self.consumer = BaseKafkaConsumer(
            bootstrap_servers=kafka_bootstrap,
            group_id="result-aggregator",
            topics=["risk-assessment-results"],
            message_handler=self.handle_assessment_result
        )
    
    def handle_assessment_result(self, result: Dict[str, Any]):
        """å¤„ç†è¯„ä¼°ç»“æœ"""
        try:
            request_id = result.get('requestId')
            camera_id = result.get('cameraId')
            
            # 1. ä»Redisè·å–åŸå§‹æ£€æµ‹ç»“æœ
            detection_key = f"detection:{request_id}"
            detection_data = self.redis.get(detection_key)
            
            if not detection_data:
                logger.warning(f"Detection data not found for {request_id}")
                return
            
            detection = json.loads(detection_data)
            
            # 2. åˆå¹¶ç»“æœ
            merged_result = {
                **detection,
                'riskAssessment': result,
                'hasDangerousDriving': result.get('hasDangerousDriving', False),
                'maxRiskLevel': result.get('maxRiskLevel', 'none'),
            }
            
            # 3. ç¼“å­˜æœ€æ–°ç»“æœ
            camera_key = f"camera:{camera_id}:latest"
            self.redis.setex(camera_key, 300, json.dumps(merged_result))  # 5åˆ†é’Ÿè¿‡æœŸ
            
            # 4. å‘å¸ƒåˆ°WebSocketé¢‘é“
            self.redis.publish(f"camera:{camera_id}", json.dumps(merged_result))
            
            # 5. è§¦å‘å‘Šè­¦
            if result.get('maxRiskLevel') == 'high':
                self._trigger_alert(camera_id, merged_result)
            
            logger.info(f"Aggregated result for camera {camera_id}, risk={result.get('maxRiskLevel')}")
        except Exception as e:
            logger.error(f"Failed to aggregate result: {e}")
    
    def _trigger_alert(self, camera_id: int, result: Dict[str, Any]):
        """è§¦å‘é«˜é£é™©å‘Šè­¦"""
        alert_key = f"alerts:{camera_id}"
        self.redis.lpush(alert_key, json.dumps(result))
        self.redis.ltrim(alert_key, 0, 99)  # ä¿ç•™æœ€è¿‘100æ¡å‘Šè­¦
        logger.warning(f"ğŸš¨ High risk alert triggered for camera {camera_id}")
    
    def start(self):
        import threading
        thread = threading.Thread(target=self.consumer.start, daemon=True)
        thread.start()
        logger.info("Result aggregator started")
```

### 4.5 ç›‘æ§ä¸å¯è§‚æµ‹æ€§

#### 4.5.1 PrometheusæŒ‡æ ‡å¯¼å‡º

**æ–‡ä»¶: `algo/monitoring/metrics.py`**
```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰æŒ‡æ ‡
detection_counter = Counter('detection_total', 'Total detections', ['camera_id'])
llm_call_counter = Counter('llm_calls_total', 'Total LLM API calls', ['key_id', 'status'])
llm_latency_histogram = Histogram('llm_latency_seconds', 'LLM API latency')
kafka_lag_gauge = Gauge('kafka_consumer_lag', 'Kafka consumer lag', ['topic', 'partition'])
api_key_status_gauge = Gauge('api_key_status', 'API key status', ['key_id', 'status'])

def record_detection(camera_id: int):
    detection_counter.labels(camera_id=camera_id).inc()

def record_llm_call(key_id: str, success: bool, latency: float):
    status = 'success' if success else 'failure'
    llm_call_counter.labels(key_id=key_id, status=status).inc()
    llm_latency_histogram.observe(latency)

def update_key_pool_metrics(key_pool):
    """æ›´æ–°API Keyæ± æŒ‡æ ‡"""
    stats = key_pool.get_stats()
    for key_stat in stats['keys']:
        for status in ['available', 'in_use', 'cooling', 'disabled']:
            value = 1 if key_stat['status'] == status else 0
            api_key_status_gauge.labels(
                key_id=key_stat['key_id'],
                status=status
            ).set(value)

# å¯åŠ¨Prometheus HTTPæœåŠ¡
def start_metrics_server(port: int = 9090):
    start_http_server(port)
    logger.info(f"Prometheus metrics server started on port {port}")
```

---

## 5. å®æ–½è®¡åˆ’

### 5.1 é˜¶æ®µåˆ’åˆ†

#### ç¬¬ä¸€é˜¶æ®µ: åŸºç¡€è®¾æ–½æ­å»º (Week 1-2)
- [ ] éƒ¨ç½²Kafkaé›†ç¾¤ (3èŠ‚ç‚¹)
- [ ] éƒ¨ç½²Flinké›†ç¾¤ (æˆ–å‡†å¤‡Pythonç¯å¢ƒ)
- [ ] éƒ¨ç½²Redisé›†ç¾¤ (ç¼“å­˜+æ¶ˆæ¯)
- [ ] éƒ¨ç½²Prometheus + Grafana
- [ ] é…ç½®ELKæ—¥å¿—æ ˆ

#### ç¬¬äºŒé˜¶æ®µ: æ ¸å¿ƒæ¨¡å—å¼€å‘ (Week 3-4)
- [ ] å®ç°Kafka Produceré›†æˆåˆ°DetectionPipeline
- [ ] å¼€å‘API Keyæ± ç®¡ç†æ¨¡å—
- [ ] å¼€å‘ä»»åŠ¡è°ƒåº¦å™¨ (LLMTaskScheduler)
- [ ] å¼€å‘Flinkä»»åŠ¡ç”Ÿæˆå™¨ (æˆ–Pythonç‰ˆæœ¬)
- [ ] å®ç°ç»“æœèšåˆæ¶ˆè´¹è€…

#### ç¬¬ä¸‰é˜¶æ®µ: é›†æˆæµ‹è¯• (Week 5)
- [ ] å•å…ƒæµ‹è¯• (å„æ¨¡å—ç‹¬ç«‹æµ‹è¯•)
- [ ] é›†æˆæµ‹è¯• (ç«¯åˆ°ç«¯æµç¨‹)
- [ ] æ€§èƒ½æµ‹è¯• (å•è·¯æ‘„åƒå¤´)
- [ ] å‹åŠ›æµ‹è¯• (10è·¯å¹¶å‘)
- [ ] æ•…éšœæ¢å¤æµ‹è¯• (Keyå¤±æ•ˆã€Kafkaå®•æœºç­‰)

#### ç¬¬å››é˜¶æ®µ: ä¼˜åŒ–ä¸ä¸Šçº¿ (Week 6)
- [ ] æ€§èƒ½è°ƒä¼˜ (å‚æ•°è°ƒæ•´)
- [ ] ç›‘æ§é¢æ¿æ­å»º
- [ ] æ–‡æ¡£ç¼–å†™
- [ ] ç°åº¦å‘å¸ƒ (5è·¯->20è·¯->50è·¯)
- [ ] å…¨é‡ä¸Šçº¿

### 5.2 ä¾èµ–å…³ç³»

```mermaid
gantt
    title å®æ–½ç”˜ç‰¹å›¾
    dateFormat  YYYY-MM-DD
    section åŸºç¡€è®¾æ–½
    Kafkaéƒ¨ç½²           :a1, 2025-10-20, 3d
    Rediséƒ¨ç½²           :a2, 2025-10-20, 2d
    ç›‘æ§éƒ¨ç½²            :a3, 2025-10-22, 2d
    section å¼€å‘
    Kafkaé›†æˆ           :b1, after a1, 3d
    API Keyæ±           :b2, 2025-10-23, 4d
    ä»»åŠ¡è°ƒåº¦å™¨          :b3, after b2, 5d
    ç»“æœæ¶ˆè´¹è€…          :b4, after b1, 3d
    section æµ‹è¯•
    é›†æˆæµ‹è¯•            :c1, after b3, 3d
    å‹åŠ›æµ‹è¯•            :c2, after c1, 2d
    section ä¸Šçº¿
    ç°åº¦å‘å¸ƒ            :d1, after c2, 3d
    å…¨é‡ä¸Šçº¿            :d2, after d1, 2d
```

---

## 6. é£é™©è¯„ä¼°ä¸åº”å¯¹

### 6.1 æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | åº”å¯¹ç­–ç•¥ |
|-----|------|------|---------|
| Kafkaæ€§èƒ½ä¸è¶³ | é«˜ | ä½ | å……åˆ†å‹æµ‹ï¼Œé¢„ç•™èµ„æºbuffer |
| Flinkå­¦ä¹ æ›²çº¿ | ä¸­ | ä¸­ | æä¾›Pythonç®€åŒ–æ–¹æ¡ˆä½œä¸ºå¤‡é€‰ |
| API Keyä¾›åº”ä¸è¶³ | é«˜ | ä¸­ | ä¸ä¾›åº”å•†åå•†ï¼Œå‡†å¤‡10+ä¸ªKey |
| ç½‘ç»œå»¶è¿Ÿæ³¢åŠ¨ | ä¸­ | é«˜ | å¢åŠ é‡è¯•æœºåˆ¶ï¼Œæœ¬åœ°ç¼“å­˜ç­–ç•¥ |
| æ¶ˆæ¯å †ç§¯ | é«˜ | ä¸­ | ç›‘æ§lagï¼Œè‡ªåŠ¨æ‰©å®¹æ¶ˆè´¹è€… |

### 6.2 ä¸šåŠ¡é£é™©

| é£é™© | å½±å“ | åº”å¯¹ç­–ç•¥ |
|-----|------|---------|
| æ£€æµ‹å»¶è¿Ÿå¢åŠ  | ä¸­ | ç›‘æ§ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œè®¾ç½®SLAå‘Šè­¦ |
| å‘Šè­¦è¯¯æŠ¥ç‡ä¸Šå‡ | ä¸­ | ä¿ç•™é™çº§å¼€å…³ï¼Œå¯å›é€€åˆ°æ—§ç‰ˆ |
| æˆæœ¬è¶…æ”¯ | ä½ | æ§åˆ¶APIè°ƒç”¨é¢‘ç‡ï¼Œè®¾ç½®é¢„ç®—ä¸Šé™ |

---

## 7. éªŒæ”¶æ ‡å‡†

### 7.1 åŠŸèƒ½éªŒæ”¶

- [x] **F1**: æ‰€æœ‰æ¨¡å—æˆåŠŸéƒ¨ç½²å¹¶å¯åŠ¨
- [ ] **F2**: æ£€æµ‹ç»“æœèƒ½æ­£ç¡®å‘é€åˆ°Kafka `detection-results` topic
- [ ] **F3**: Flink/Pythonä»»åŠ¡ç”Ÿæˆå™¨èƒ½æ­£å¸¸æ¶ˆè´¹å¹¶ç”Ÿæˆè¯„ä¼°ä»»åŠ¡
- [ ] **F4**: ä»»åŠ¡è°ƒåº¦å™¨èƒ½ä½¿ç”¨æ‰€æœ‰é…ç½®çš„API Keyè¿›è¡Œå¹¶å‘è°ƒç”¨
- [ ] **F5**: è¯„ä¼°ç»“æœèƒ½æ­£ç¡®å‘é€åˆ° `risk-assessment-results` topic
- [ ] **F6**: ä¸‹æ¸¸æ¶ˆè´¹è€…èƒ½æ­£ç¡®èšåˆç»“æœå¹¶æ¨é€ç»™WebSocket

### 7.2 æ€§èƒ½éªŒæ”¶

- [ ] **P1**: æ”¯æŒè‡³å°‘ **50è·¯æ‘„åƒå¤´** åŒæ—¶å¤„ç†
- [ ] **P2**: ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆä»ç”»é¢æ•è·åˆ°ç»“æœæ¨é€ï¼‰ **< 2ç§’** (P95)
- [ ] **P3**: APIè°ƒç”¨æˆåŠŸç‡ **> 99%**
- [ ] **P4**: ç³»ç»Ÿæ•´ä½“ååé‡ç›¸æ¯”æ—§ç‰ˆæå‡ **10å€ä»¥ä¸Š**
- [ ] **P5**: Kafkaæ¶ˆæ¯å †ç§¯ (lag) **< 1000æ¡**

### 7.3 ç¨³å®šæ€§éªŒæ”¶

- [ ] **S1**: å•ä¸ªAPI Keyå¤±æ•ˆï¼Œç³»ç»Ÿèƒ½è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶ä»–Key
- [ ] **S2**: Kafkaå•èŠ‚ç‚¹å®•æœºï¼Œç³»ç»Ÿèƒ½ç»§ç»­è¿è¡Œ
- [ ] **S3**: è°ƒåº¦å™¨è¿›ç¨‹é‡å¯ï¼Œæœªå¤„ç†ä»»åŠ¡ä¸ä¸¢å¤±
- [ ] **S4**: å‹æµ‹2å°æ—¶æ— å†…å­˜æ³„æ¼
- [ ] **S5**: 7x24å°æ—¶ç¨³å®šè¿è¡Œ

### 7.4 å¯è§‚æµ‹æ€§éªŒæ”¶

- [ ] **O1**: Prometheusé‡‡é›†åˆ°æ‰€æœ‰å…³é”®æŒ‡æ ‡
- [ ] **O2**: Grafanaé¢æ¿å±•ç¤ºå®æ—¶ç›‘æ§æ•°æ®
- [ ] **O3**: æ—¥å¿—èƒ½æŸ¥è¯¢åˆ°æ¯æ¬¡APIè°ƒç”¨è®°å½•
- [ ] **O4**: å‘Šè­¦èƒ½åŠæ—¶è§¦å‘ (å»¶è¿Ÿ>5ç§’ã€é”™è¯¯ç‡>5%)

---

## 8. é™„å½•

### 8.1 é…ç½®æ–‡ä»¶ç¤ºä¾‹

**`config/kafka.yaml`**
```yaml
kafka:
  bootstrap_servers: "kafka-1:9092,kafka-2:9092,kafka-3:9092"
  topics:
    detection_results: "detection-results"
    assessment_tasks: "assessment-tasks"
    risk_assessment_results: "risk-assessment-results"
  producer:
    compression_type: "snappy"
    acks: 1
    linger_ms: 10
    batch_size: 32768
  consumer:
    group_id_prefix: "traffic-monitor"
    auto_offset_reset: "latest"
    enable_auto_commit: true
```

**`config/api_keys.yaml`**
```yaml
api_keys:
  - key: "sk-xxx1"
    qps_limit: 10
    rpm_limit: 300
  - key: "sk-xxx2"
    qps_limit: 10
    rpm_limit: 300
  - key: "sk-xxx3"
    qps_limit: 10
    rpm_limit: 300
  # ... æ›´å¤šKey

scheduler:
  max_concurrent_tasks: 50
  key_cooldown_seconds: 60
  retry_max_attempts: 2
```

### 8.2 Docker Composeç¤ºä¾‹

**`docker-compose.yml`**
```yaml
version: '3.8'

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
  
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
  
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
  
  flink-jobmanager:
    image: flink:1.18-python3.11
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
  
  flink-taskmanager:
    image: flink:1.18-python3.11
    command: taskmanager
    depends_on:
      - flink-jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
```

---

## æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡å¼•å…¥ **Kafkaæ¶ˆæ¯é˜Ÿåˆ—** è§£è€¦ç³»ç»Ÿå„æ¨¡å—ï¼Œé€šè¿‡ **Flink/Pythonä»»åŠ¡ç”Ÿæˆå™¨** å®ç°æµå¼å¤„ç†ï¼Œé€šè¿‡ **API Keyæ± åŒ–è°ƒåº¦å™¨** çªç ´å•Key QPSé™åˆ¶ï¼Œæ„å»ºäº†ä¸€ä¸ª **é«˜å¹¶å‘ã€ä½å»¶è¿Ÿã€é«˜å¯ç”¨** çš„å®æ—¶å±é™©ç”»é¢æ£€æµ‹ç³»ç»Ÿã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
1. âœ… å¹¶å‘èƒ½åŠ›æå‡10å€ä»¥ä¸Š
2. âœ… ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½åˆ°2ç§’ä»¥å†…
3. âœ… æ”¯æŒæ°´å¹³æ‰©å±• (å¢åŠ Key/å®ä¾‹)
4. âœ… é«˜å®¹é”™æ€§ (å•ç‚¹æ•…éšœä¸å½±å“æ•´ä½“)
5. âœ… å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»

**ä¸‹ä¸€æ­¥ï¼š** å¼€å§‹ç¬¬ä¸€é˜¶æ®µçš„åŸºç¡€è®¾æ–½éƒ¨ç½²å·¥ä½œã€‚
