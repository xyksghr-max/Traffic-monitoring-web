# å¹¶å‘ä¼˜åŒ–ç³»ç»Ÿå¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ å‰ææ¡ä»¶

### ç¯å¢ƒè¦æ±‚
- Python 3.10+
- Docker & Docker Compose
- è‡³å°‘ 16GB å†…å­˜
- å¤šä¸ª DashScope API Key (å»ºè®® 5-10 ä¸ª)

### å·²å®‰è£…æœåŠ¡
- Kafka é›†ç¾¤ (æˆ–ä½¿ç”¨ Docker Compose)
- Redis (ç”¨äºç»“æœç¼“å­˜)
- Prometheus & Grafana (å¯é€‰ï¼Œç”¨äºç›‘æ§)

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨æ­¥éª¤

### Step 1: å¯åŠ¨åŸºç¡€è®¾æ–½

ä½¿ç”¨ Docker Compose ä¸€é”®å¯åŠ¨ Kafkaã€Redis ç­‰æœåŠ¡ï¼š

```bash
cd /workspaces/Traffic-monitoring-web
docker-compose -f deployment/docker-compose.infra.yml up -d
```

ç­‰å¾…æœåŠ¡å¯åŠ¨å®Œæˆï¼š
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose -f deployment/docker-compose.infra.yml ps
```

### Step 2: å®‰è£… Python ä¾èµ–

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate     # Windows

# å®‰è£…æ–°å¢ä¾èµ–
pip install -r requirements-streaming.txt
```

`requirements-streaming.txt` åŒ…å«:
```txt
confluent-kafka>=2.3.0
apache-flink>=1.18.0
aiohttp>=3.9.0
redis>=5.0.0
prometheus-client>=0.19.0
```

### Step 3: é…ç½® API Keys

ç¼–è¾‘ `config/api_keys.yaml`:

```yaml
api_keys:
  - key: "sk-your-key-1"
    qps_limit: 10
    rpm_limit: 300
    enabled: true
  
  - key: "sk-your-key-2"
    qps_limit: 10
    rpm_limit: 300
    enabled: true
  
  - key: "sk-your-key-3"
    qps_limit: 10
    rpm_limit: 300
    enabled: true

scheduler:
  max_concurrent_tasks: 50
  key_cooldown_seconds: 60
  retry_max_attempts: 2
  timeout_seconds: 35
```

### Step 4: é…ç½® Kafka è¿æ¥

ç¼–è¾‘ `config/kafka.yaml`:

```yaml
kafka:
  bootstrap_servers: "localhost:9092"
  
  topics:
    detection_results: "detection-results"
    assessment_tasks: "assessment-tasks"
    risk_assessment_results: "risk-assessment-results"
  
  producer:
    compression_type: "snappy"
    acks: 1
    linger_ms: 10
    batch_size: 32768
  
  consumer:
    group_id_prefix: "traffic-monitor"
    auto_offset_reset: "latest"
    enable_auto_commit: true
    auto_commit_interval_ms: 5000
```

### Step 5: åˆå§‹åŒ– Kafka Topics

```bash
python scripts/init_kafka_topics.py
```

è¯¥è„šæœ¬ä¼šåˆ›å»ºå¿…è¦çš„ Kafka Topicsï¼š
- `detection-results` (16 åˆ†åŒº)
- `assessment-tasks` (16 åˆ†åŒº)
- `risk-assessment-results` (16 åˆ†åŒº)

### Step 6: å¯åŠ¨æœåŠ¡ç»„ä»¶

**ç»ˆç«¯ 1: å¯åŠ¨æ£€æµ‹æœåŠ¡ (åŸæœ‰æœåŠ¡ + Kafka Producer)**
```bash
python app.py
```

**ç»ˆç«¯ 2: å¯åŠ¨ä»»åŠ¡ç”Ÿæˆå™¨ (å¯é€‰ Flink æˆ– Python)**
```bash
# æ–¹æ¡ˆ A: ä½¿ç”¨ Python ç®€åŒ–ç‰ˆ
python -m algo.task_generator.simple_generator

# æ–¹æ¡ˆ B: ä½¿ç”¨ Flink (éœ€å…ˆå®‰è£… Flink)
flink run -py flink_jobs/assessment_task_generator.py
```

**ç»ˆç«¯ 3: å¯åŠ¨ LLM è°ƒåº¦å™¨**
```bash
python -m algo.scheduler.task_scheduler
```

**ç»ˆç«¯ 4: å¯åŠ¨ç»“æœèšåˆæ¶ˆè´¹è€…**
```bash
python -m algo.consumers.result_aggregator
```

### Step 7: éªŒè¯ç³»ç»Ÿè¿è¡Œ

#### 7.1 æ£€æŸ¥ Kafka æ¶ˆæ¯æµ

```bash
# æŸ¥çœ‹æ£€æµ‹ç»“æœ topic
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic detection-results --from-beginning --max-messages 1

# æŸ¥çœ‹è¯„ä¼°ä»»åŠ¡ topic
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic assessment-tasks --from-beginning --max-messages 1

# æŸ¥çœ‹é£é™©è¯„ä¼°ç»“æœ topic
kafka-console-consumer --bootstrap-server localhost:9092 \
  --topic risk-assessment-results --from-beginning --max-messages 1
```

#### 7.2 æ£€æŸ¥ API Key æ± çŠ¶æ€

è®¿é—®ç›‘æ§ç«¯ç‚¹ï¼š
```bash
curl http://localhost:5000/api/scheduler/stats
```

é¢„æœŸå“åº”ï¼š
```json
{
  "code": 200,
  "data": {
    "total_keys": 3,
    "available_keys": 3,
    "in_use_keys": 0,
    "cooling_keys": 0,
    "total_calls": 145,
    "success_rate": 0.986,
    "keys": [
      {
        "key_id": "key_1",
        "status": "available",
        "total_calls": 48,
        "success_rate": 0.979
      },
      ...
    ]
  }
}
```

#### 7.3 æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š
```bash
python scripts/test_streaming_pipeline.py --camera-id 1 --duration 60
```

è¯¥è„šæœ¬ä¼šï¼š
1. å¯åŠ¨ä¸€ä¸ªæµ‹è¯•æ‘„åƒå¤´
2. ç›‘æ§æ¶ˆæ¯æµè½¬
3. éªŒè¯ç«¯åˆ°ç«¯å»¶è¿Ÿ
4. è¾“å‡ºæ€§èƒ½æŠ¥å‘Š

---

## ğŸ“Š ç›‘æ§é¢æ¿

### Prometheus æŒ‡æ ‡

è®¿é—® `http://localhost:9090`ï¼ŒæŸ¥è¯¢ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **æ£€æµ‹æ€»æ•°**: `detection_total`
- **LLM è°ƒç”¨æ€»æ•°**: `llm_calls_total{status="success"}`
- **LLM å»¶è¿Ÿåˆ†å¸ƒ**: `llm_latency_seconds`
- **Kafka æ¶ˆæ¯å †ç§¯**: `kafka_consumer_lag`
- **API Key çŠ¶æ€**: `api_key_status`

### Grafana ä»ªè¡¨ç›˜

è®¿é—® `http://localhost:3000` (é»˜è®¤è´¦å·: admin/admin)

å¯¼å…¥é¢„é…ç½®ä»ªè¡¨ç›˜ï¼š
```bash
# å¯¼å…¥ Grafana é…ç½®
curl -X POST http://admin:admin@localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @deployment/grafana-dashboard.json
```

---

## ğŸ§ª å‹åŠ›æµ‹è¯•

### å•è·¯æ‘„åƒå¤´æµ‹è¯•

```bash
python scripts/benchmark.py \
  --cameras 1 \
  --duration 300 \
  --report-file reports/single_camera.json
```

### å¤šè·¯å¹¶å‘æµ‹è¯•

```bash
python scripts/benchmark.py \
  --cameras 50 \
  --duration 600 \
  --report-file reports/50_cameras.json
```

### æ€§èƒ½æŒ‡æ ‡æ”¶é›†

æµ‹è¯•æŠ¥å‘Šä¼šåŒ…å«ï¼š
- å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿ
- P50/P95/P99 å»¶è¿Ÿåˆ†å¸ƒ
- ååé‡ (å¸§/ç§’)
- API è°ƒç”¨æˆåŠŸç‡
- Kafka æ¶ˆæ¯å †ç§¯æƒ…å†µ

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Kafka è¿æ¥å¤±è´¥

**ç—‡çŠ¶**: æ—¥å¿—æ˜¾ç¤º `Failed to connect to Kafka`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ Kafka æœåŠ¡çŠ¶æ€
docker ps | grep kafka

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
telnet localhost 9092

# é‡å¯ Kafka
docker-compose -f deployment/docker-compose.infra.yml restart kafka
```

### é—®é¢˜ 2: API Key å…¨éƒ¨è¿›å…¥å†·å´çŠ¶æ€

**ç—‡çŠ¶**: æ—¥å¿—æ˜¾ç¤º `Failed to acquire API key within timeout`

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆ:
   ```bash
   python scripts/verify_api_keys.py
   ```

2. è°ƒæ•´å†·å´æ—¶é—´ (åœ¨ `config/api_keys.yaml`):
   ```yaml
   scheduler:
     key_cooldown_seconds: 30  # ä» 60 å‡å°‘åˆ° 30
   ```

3. å¢åŠ æ›´å¤š API Key

### é—®é¢˜ 3: æ¶ˆæ¯å †ç§¯ä¸¥é‡

**ç—‡çŠ¶**: Kafka lag æŒç»­å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è°ƒåº¦å™¨å¹¶å‘æ•°:
   ```yaml
   scheduler:
     max_concurrent_tasks: 100  # ä» 50 æå‡åˆ° 100
   ```

2. å¯åŠ¨å¤šä¸ªè°ƒåº¦å™¨å®ä¾‹:
   ```bash
   # ç»ˆç«¯ 1
   python -m algo.scheduler.task_scheduler --instance-id scheduler-1
   
   # ç»ˆç«¯ 2
   python -m algo.scheduler.task_scheduler --instance-id scheduler-2
   ```

3. å¢åŠ  Kafka åˆ†åŒºæ•°:
   ```bash
   kafka-topics --bootstrap-server localhost:9092 \
     --alter --topic assessment-tasks --partitions 32
   ```

### é—®é¢˜ 4: å†…å­˜å ç”¨è¿‡é«˜

**ç—‡çŠ¶**: ç³»ç»Ÿå†…å­˜æŒç»­å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
1. é™åˆ¶æ¶ˆæ¯ç¼“å­˜å¤§å° (åœ¨ `config/kafka.yaml`):
   ```yaml
   consumer:
     max_poll_records: 100  # é™åˆ¶æ¯æ¬¡æ‹‰å–çš„æ¶ˆæ¯æ•°
   ```

2. è°ƒæ•´ Python GC:
   ```python
   import gc
   gc.set_threshold(700, 10, 5)  # æ›´é¢‘ç¹çš„åƒåœ¾å›æ”¶
   ```

3. ä½¿ç”¨å¯¹è±¡æ± å‡å°‘å†…å­˜åˆ†é…

---

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. Kafka ä¼˜åŒ–

**ç”Ÿäº§è€…é…ç½®**:
```yaml
producer:
  compression_type: "snappy"  # å‹ç¼©å‡å°‘ç½‘ç»œä¼ è¾“
  linger_ms: 10               # æ‰¹å¤„ç†å»¶è¿Ÿ
  batch_size: 65536           # å¢å¤§æ‰¹å¤„ç†å¤§å° (ä» 32KB åˆ° 64KB)
  buffer_memory: 67108864     # 64MB ç¼“å†²åŒº
  acks: 1                     # å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§
```

**æ¶ˆè´¹è€…é…ç½®**:
```yaml
consumer:
  fetch_min_bytes: 1024       # æœ€å°æ‹‰å–å¤§å°
  fetch_max_wait_ms: 500      # æœ€å¤§ç­‰å¾…æ—¶é—´
  max_poll_records: 500       # æ¯æ¬¡æ‹‰å–è®°å½•æ•°
```

### 2. API Key æ± ä¼˜åŒ–

**å¢åŠ  Key æ•°é‡**:
- ç†æƒ³æ¯”ä¾‹: 1 ä¸ª Key æ”¯æŒ 5-10 è·¯æ‘„åƒå¤´
- 50 è·¯æ‘„åƒå¤´å»ºè®® 10-15 ä¸ª Key

**è°ƒæ•´å†·å´ç­–ç•¥**:
```yaml
scheduler:
  key_cooldown_seconds: 30    # ç¼©çŸ­å†·å´æ—¶é—´
  adaptive_cooldown: true     # å¯ç”¨è‡ªé€‚åº”å†·å´
  min_cooldown: 10
  max_cooldown: 120
```

### 3. å¹¶å‘æ§åˆ¶

**è°ƒåº¦å™¨å¹¶å‘**:
```yaml
scheduler:
  max_concurrent_tasks: 100   # æ ¹æ®ç¡¬ä»¶è°ƒæ•´
  task_timeout_seconds: 40    # ä»»åŠ¡è¶…æ—¶
  queue_size: 500             # ä»»åŠ¡é˜Ÿåˆ—å¤§å°
```

### 4. ç›‘æ§å‘Šè­¦é˜ˆå€¼

åœ¨ `config/monitoring.yaml` ä¸­è®¾ç½®:
```yaml
alerts:
  latency_threshold_p95: 2.5  # P95 å»¶è¿Ÿè¶…è¿‡ 2.5 ç§’å‘Šè­¦
  error_rate_threshold: 0.05  # é”™è¯¯ç‡è¶…è¿‡ 5% å‘Šè­¦
  kafka_lag_threshold: 1000   # æ¶ˆæ¯å †ç§¯è¶…è¿‡ 1000 å‘Šè­¦
  api_key_failure_threshold: 0.2  # å• Key å¤±è´¥ç‡è¶…è¿‡ 20% å‘Šè­¦
```

---

## ğŸ”„ å‡çº§ä¸å›æ»š

### å‡çº§åˆ°æ–°ç‰ˆæœ¬

```bash
# 1. å¤‡ä»½é…ç½®
cp -r config config.backup

# 2. æ‹‰å–æœ€æ–°ä»£ç 
git pull origin main

# 3. æ›´æ–°ä¾èµ–
pip install -r requirements-streaming.txt --upgrade

# 4. æ•°æ®åº“è¿ç§» (å¦‚æœ‰)
python scripts/migrate_schema.py

# 5. æ»šåŠ¨é‡å¯æœåŠ¡
# å…ˆé‡å¯è°ƒåº¦å™¨ (é›¶åœæœº)
systemctl restart llm-scheduler-1
sleep 30
systemctl restart llm-scheduler-2

# å†é‡å¯æ£€æµ‹æœåŠ¡
systemctl restart detection-pipeline
```

### ç´§æ€¥å›æ»šåˆ°æ—§ç‰ˆæœ¬

```bash
# 1. åœæ­¢æ‰€æœ‰æ–°æœåŠ¡
docker-compose -f deployment/docker-compose.streaming.yml down

# 2. æ¢å¤æ—§ç‰ˆæœ¬ä»£ç 
git checkout <previous-commit-hash>

# 3. æ¢å¤é…ç½®
cp -r config.backup config

# 4. é‡å¯æ—§ç‰ˆæœåŠ¡
python app.py
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ¶æ„ä¼˜åŒ–å®æ–½æ–¹æ¡ˆ](./æ¶æ„ä¼˜åŒ–å®æ–½æ–¹æ¡ˆ.md) - è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ
- [API æ–‡æ¡£](./APIæ–‡æ¡£.md) - æ¥å£å®šä¹‰
- [è¿ç»´æ‰‹å†Œ](./è¿ç»´æ‰‹å†Œ.md) - éƒ¨ç½²å’Œç»´æŠ¤æŒ‡å—
- [æ•…éšœæ’æŸ¥æ‰‹å†Œ](./æ•…éšœæ’æŸ¥æ‰‹å†Œ.md) - å¸¸è§é—®é¢˜è§£å†³

---

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿå°è¯•ä»¥ä¸‹æ–¹å¼ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**:
   ```bash
   # åº”ç”¨æ—¥å¿—
   tail -f logs/app.log
   
   # è°ƒåº¦å™¨æ—¥å¿—
   tail -f logs/scheduler.log
   
   # Kafka æ—¥å¿—
   docker logs -f kafka
   ```

2. **æ£€æŸ¥å¥åº·çŠ¶æ€**:
   ```bash
   curl http://localhost:5000/api/health/health_check
   curl http://localhost:5000/api/scheduler/health
   ```

3. **æäº¤ Issue**: [GitHub Issues](https://github.com/xyksghr-max/Traffic-monitoring-web/issues)

4. **æŸ¥çœ‹ç›‘æ§é¢æ¿**: http://localhost:3000

---

**ç¥éƒ¨ç½²é¡ºåˆ©ï¼ğŸ‰**
